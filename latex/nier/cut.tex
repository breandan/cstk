%! Author = breandanconsidine
%! Date = 10/13/21

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document
\begin{document}
  \noindent\begin{lstlisting}[basicstyle=\footnotesize\ttfamily, language=kotlin,label={lst:lstlisting}]
fun evaluate(ncc, snps, msk, ctx, mtr) = Δ(
  zip(snps, snps | msk | ncc) | mtr | average,
  zip(snps | ctx, snps | ctx | msk | ncc) | mtr | average
)
  \end{lstlisting}

  \noindent where \texttt{|} maps a function over a sequence, and \lstinline|zip| zips two sequences into a sequence of pairs. We assume \lstinline|snps| and \lstinline|msk| are fixed, and evaluate three neural code completion models on five different source code transformations (SCTs) and two downstream tasks. Prior work has explored similar probing tasks in the natural language setting, however due to the clear demarcation between syntax and semantics in programming languages, one can more precisely synthesize and reason about semantically admissible transformations, a task which is considerably more difficult in natural languages due to the problem of semantic drift.


%
%  For a given query and context, we first compute the context embedding and using a distance metric, fetch the k-nearest neighboring documents in latent space, forming the depth-1 nodes in our graph, then repeat this procedure recursively, pruning with a beam search heuristic based on the total path length in latent space. This procedure is depicted in Fig.~\ref{fig:de2kg}.
%
%  \begin{figure}[H]
%    \centering
%    \includegraphics[width=0.4\textwidth]{figs/architecture}
%    \caption{Unlike language models which directly learn the data distribution, our model is designed to query an unseen database only available at test time. The model scores and selectively expands a subset of promising results within the database using a beam search heuristic, then runs message passing over the resulting graph to obtain the final task prediction.}
%    \label{fig:architecture}
%  \end{figure}
%
%  Consider the depth-1 beam search procedure (Fig. ~\ref{fig:pipeline}): We can use either vector search or keyword search to perform the context expansion, although vector search has the disadvantage of needing to rebuild the vector embedding index at each step during training. Keyword search is comparatively cheaper, as it only needs to build once for each repo on MiniGithub.
%
%  \begin{figure}[H]
%    \centering
%    \includegraphics[width=0.4\textwidth]{figs/knn_vs_vec}
%    \caption{In the vector search procedure, we compute an embedding for the masked source context, then retrieve the k-nearest sequence embeddings in our database. In the keyword search, we compute an n-hot vector to select the keywords, and retrieve the nearest matching sequences in our index. In both cases, we embed the results, feed them into a transformer, compute a loss between the predicted output and the masked token, and backpropagate.}
%    \label{fig:pipeline}
%  \end{figure}
%
%  Once the beam search procedure is complete, we have a graph representing the neighborhood of the nearest neighboring code fragments in the parent project. This forms a so-called \textit{virtual knowledge graph}, with nodes decorated by the CodeBERT sequence embeddings and edges decorated with the direction vector between documents. We then run $p$ rounds of message passing to obtain graph embedding or \textit{neighborhood summary vector} (Fig.~\ref{fig:architecture}).

% We initialize the policy network using a pretrained language model. Starting at the site of the prediction task and conditioning on its local context, the policy network draws K queries from its latent state, which are fed into the index to produce a set of matching locations. The model scores and selectively expands a subset of those locations, and the process is repeated using finite-horizon MCTS or similar beam search procedure to retrieve a set of contextually relevant locations within the parent project.
%
%The rollout traces form a graph of related locations inside each project and their corresponding context embeddings, which together form the GNN node features. Once the rollout ends, we run message passing on the resulting GNN for a fixed numbers of steps and decode the graph embedding to obtain a task prediction. The decoder, GNN parameters, and policy network are all trained end-to-end on the downstream task, e.g. code completion, defect detection or correction. After convergence, we compare the results across horizon size and analyze the queries and filetypes which are selected

%  \pagebreak
%  \section{Experiments}\label{sec:experiments}
%
%  In this work, we attempt to understand the relationship between entities in a software project. Our research seeks to answer the following questions:
%
%  \begin{enumerate}
%    \item Which contexts in a software project share mutual information?
%    \item To what degree can we claim the model has learned to:\begin{enumerate}
%                                                                 \item Locate contextually relevant artifacts within a software project?
%%   \item Comprehend the semantic content of the artifacts traversed?
%%   \item Apply the knowledge gathered to perform the assigned task?
%    \end{enumerate}
%  \end{enumerate}
%
%  For each of these models (available on HuggingFace), we sample a set of code snippets from our synthetic dataset, and compare how well they agree on each task.

%  In contrast with classical code completion models which only require a file-local context, our method is designed to navigate an entire project. In the following experiments, we compare completion accuracy with a vanilla sequence prediction model, as well as an AST-structured sequence prediction model trained from a corpus of Java projects on the same task.
%
%  We hypothesize that by jointly learning to choose locations in a project over which to attend while solving a downstream task, such as masked sequence completion, our model will produce a feature representation capable of locating and extracting information from semantically relevant contexts. We evaluate our hypothesis both qualitatively and quantitatively.
%
%  In our first set of experiments, we try to understand what is shared between sequences mapped to similar latent vectors. Do similar sequences share salient keywords? Are those keywords relevant to the task?
%
%  In our second experiment, we try to measure the information gain from including and excluding various filetypes through ablation. For graphs containing filetypes which include Markdown or Java, what kinds of information do these resources provide and which categories are most salient?

%In our third experiment, we compare prediction accuracy across architectures and datasets. Can we constrain the action space (e.g. only querying tokens from the surrounding context) for more efficient trajectory sampling, or allow arbitrary queries? How well does the model architecture transfer to new repositories, within and across programming languages?

%  In our third and final set of experiments, we compare performance across hyperparameters. Does contextual expansion lead to better task performance for a given sequence prediction task? By relaxing edge-construction criteria and increasing hyperparamers such as beam search budget, we would expect corresponding task performance to increase.
%
%  If our hypothesis is correct, the virtual knowledge graph will span both natural language and source code artifacts. If so, this would provide evidence to support the broader hypothesis~\citep{guo2017semantically} that documentation is a useful source of information. In addition to being useful for the prediction task itself, we anticipate our model could also be used for knowledge graph extraction and suggest semantically relevant code snippets to developers.

% For example, suppose we are given a string of text in a foreign language:

% Ribbit bleep bloop ribbet

% Perhaps the “bleep bloop” is a proper name, which could be replaced with any other without changing the meaning:

% Ribbit flip flop ribbet

% Or perhaps flip/flop changes the meaning drastically. How do we know from examples alone?

%   Imagine that we have a stochastic language model that takes a sequence and emits a distribution over sequences:

%   $LM: \seqWord  \rightarrow \distSeqWord $

%   But how do we obtain LM in the first place? A good language metamodel then, would produce language models that tend to generalize, or align with user preferences:

%   $LMM: \distSeqWord \rightarrow \texttt{LM}$

%   For many computational languages, the number of models that explain our data far outnumber the size of the dataset, e.g. depending on the language complexity, there can be an exponential, or super-exponential number of models that exactly describe the data. If we wish to consider models that approximate the data, the number of models is effectively infinite. How do we form a distribution over models we cannot even enumerate? For small trees, it may be possible to perform beam-search, but for any reasonably sized query, we simply cannot enumerate all possibilities.

%   Furthermore, models typically average across all authors, needs, intents and situations. Language model designers are forced to compress human needs into the model training algorithm, which does not work very well. Effectively, we select a higher-order model which produces models that tend to align with human-selected priors. Since these priors are often hard to translate, there must be more human input during the design process. Most of these preferences are incorporated implicitly during the meta model design phase in the dataset selection and feature design.

%   $LMM: (Intent) \times (Training Data) \rightarrow LM$

%   Rather than push all the needs and intent into the model training, we argue that these needs should be incorporated into the model at inference time. This is essentially the goal of meta learning, to factorize these probabilities into separate inputs.

%   $LM: \seqWord \times (Intent) \rightarrow \distSeqWord$

%   If we want to be able to make better code assistants, we need to incorporate the preferences of the human at runtime. To develop more reusable and human adaptive models, we need a way to train a model that can incorporate the user’s preferences at runtime. Instead of trying to anticipate the users needs a priori, we need to be able to produce a model that can take some input from the user (say a query) and emit an answer:

%   $LM: ( \seqWord ) \times (Query) \rightarrow \distSeqWord $

% So the user can feed a query and depending on the query, the model will adjust the output distribution depending on its contents.

  \usepackage{listings}%! suppress = LineBreak
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
  \documentclass[sigconf,review,anonymous]{acmart}


% Packages
  \usepackage{amsfonts}
  \usepackage{amsmath}
  \usepackage{graphicx}
  \usepackage{hyperref}
  \usepackage{float}
  \usepackage{pgfplots}
  \usepgfplotslibrary{fillbetween}
  \usepackage[pdf]{graphviz}
  \usepackage{tikz}
  \usepackage{natbib}

  \usepackage{booktabs}
  \usepackage{pifont}
  \usepackage{fontspec}
  \usepackage{fontawesome}

  \newcommand{\wmark}{\textcolor{orange}{\ding{45}}}
  \newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
  \newcommand{\xmark}{\textcolor{red}{\ding{55}}}

  \usepackage{multicol}

% Packages
  \usepackage{amsmath}
  \usepackage{listings}
  \usepackage{fontspec}
  \usepackage{xcolor}

  \setmonofont{JetBrainsMono}[
    Contextuals={Alternate},
    Path=./JetbrainsFontFiles/,
    Extension = .ttf,
    UprightFont=*-Regular,
    BoldFont=*-Bold,
    ItalicFont=*-Italic,
    BoldItalicFont=*-BoldItalic
  ]

  \makeatletter
  \def\verbatim@nolig@list{}
  \makeatother

  \lstdefinelanguage{kotlin}{
    comment=[l]{//},
    commentstyle={\color{gray}\ttfamily},
    emph={delegate, filter, firstOrNull, forEach, it, lazy, mapNotNull, println, repeat, assert, with, head, tail, len, return@},
    numberstyle=\noncopyable,
    emphstyle={\color{OrangeRed}},
    identifierstyle=\color{black},
    keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, infix, interface, internal, is, null, object, open, operator, override, package, private, public, return, sealed, set, super, suspend, this, throw, true, try, catch, typealias, val, var, vararg, when, where, while, tailrec, reified},
    keywordstyle={\color{NavyBlue}\bfseries},
    morecomment=[s]{/*}{*/},
    morestring=[b]",
    morestring=[s]{"""*}{*"""},
    ndkeywords={@Deprecated, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Boolean, Int, Integer, Iterable, Long, Runnable, Short, String},
    ndkeywordstyle={\color{BurntOrange}\bfseries},
    sensitive=true,
    stringstyle={\color{ForestGreen}\ttfamily},
    literate={`}{{\char0}}1
  }

  \lstset{basicstyle=\ttfamily\lst@ifdisplaystyle\small\fi}

%% NOTE that a single column version may be required for
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to
%% \documentclass[manuscript,screen]{acmart}
%%
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages
%% before creating your document. The white list page provides
%% information on how to submit additional LaTeX packages for
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
  \AtBeginDocument{%
    \providecommand\BibTeX{{%
      \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
  \setcopyright{acmcopyright}
  \copyrightyear{2022}
  \acmYear{2022}
  \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.


  \acmConference[ICSE 2022]{The 44th International Conference on Software Engineering}{May 21–29, 2022}{Pittsburgh, PA, USA}

  \input{macros}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
  \begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
    \title{How robust is neural code completion to cosmetic variance?}
% \title{Code Transformations
% Neural language models
% Interpretability
% Reliability/Robustness
% Comprehension/Understanding

% }

% \title{An empirical study of source code transformations on the robustness of neural language models}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
    \author{Breandan Considine, Xujie Si, Jin L.C. Guo}
    \email{breandan.considine@mail.mcgill.ca, {xsi, jguo}@cs.mcgill.ca}
    \affiliation{%
      \institution{McGill University}
    }

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
    \begin{abstract}
      Neural language models hold great promise as tools for computer-aided programming, but questions remain over their reliability and the consequences of overreliance. In the domain of natural language, prior work has revealed these models can be sensitive to naturally-occurring variance and malfunction in unpredictable ways. A closer examination of neural language models is needed to understand their behavior in programming-related tasks. In this work, we develop a methodology for systematically evaluating neural code completion models using common source code transformations such as synonymous renaming, intermediate logging, and independent statement reordering. Applying these synthetic transformations to a dataset of handwritten code snippets, we evaluate three SoTA models, CodeBERT, GraphCodeBERT and RobertA-Java, which exhibit varying degrees of robustness to cosmetic variance. Our approach is implemented and released as a modular and extensible toolkit for evaluating code-based neural language models.


      % What is the key message?
      %   - Different types of SCTs
      %.  - Different types of deep learning models (e.g. RNNs, transformers)
      %   - Different types of Evaluation tasks (e.g. code completion)
      %.  - Different measures of robustness (e.g. latent space, predictions)
      %
    \end{abstract}

    \maketitle

    \section{Introduction}\label{sec:introduction}

    Together, these transformations represent ``possible worlds'' in the tradition modal logic: the original author plausibly could have chosen an alternate form of expressing the same procedure. Although we are unable to access all these worlds, we may be able to posit the existence and likelihood of cosmetic alternatives, given a large enough dataset of sufficiently similar code and further reason about the completion model's predictions under those alternatives.

    Source code for semantically or even syntactically-equivalent programs has many degrees of freedom: two authors implementing the same function may select different variable names or cosmetic features, such as whitespaces, diagnostic statements or comments. One would expect neural code completion for languages semantically invariant under those changes to exhibit the same invariance: \textit{cosmetically-altered code snippets should not drastically change the model's predictions}. For example, code completion in Java should be invariant to variable renaming, whitespace placement, extra documentation or reordering dataflow-independent statements.


    \pagebreak\section{Method}

    Given a set of source code snippets of varying complexity, a set of SCTs, and a set of models, how do these factors affect completion accuracy? To measure this, we first sample the top 100 most-starred Java repositories published by GitHub organizations with over 100 forks and between 1 and 10 MB in size. This ensures a diverse dataset of active repositories with reasonable quality and stylistic diversity. From these, we extract a set of Java methods using the heuristic described in \S\ref{sec:slicing}.

    Our goal is to measure the robustness of SoTA neural code completion models on natural code snippets exposed to various cosmetic transformations. To do so, we first construct one SCT from each of the following five categories of cosmetic changes:

    \begin{enumerate}
      \item \textbf{Synonym renaming}: renames variables with synonyms
      \item \textbf{Peripheral code}: introduces dead code to source code
      \item \textbf{Statement reordering}: swaps independent statements
      \item \textbf{Permute argument order}: scrambles method arguments
      \item \textbf{Document generation}: adds synthetic documentation
    \end{enumerate}

    Ideally, these SCTs would be implemented using a higher-order abstract syntax (HOAS) to ensure syntactic validity, however for the sake of simplicity, we implemented the transformations using a set of ad-hoc regular expressions (regexes). While slightly clumsy for more complex SCTs, we observed that regex-based pattern matching can reliably perform cosmetic treatments like renaming and linear statement reordering without much difficulty. Specifically, we have implemented our SCTs as follows:

    \begin{enumerate}
      \item The \lstinline|renameTokens| SCT substitutes each CamelCase subword in the most frequent user-defined token with a uniformly-sampled lemma from its WordNet hypernym ego graph up to three hops away, representing an alternately-chosen (e.g., variable or function) name of similar meaning.
      \item The \lstinline|addExtraLogging| SCT adds intermittent print statements in linear chains of code, with a single argument synthesized by the code completion model for added variation. More generally, this can be any superfluous statement which does not change the runtime semantics.
      \item The \lstinline|swapMultilineNo| SCT swaps adjacent lines of equal scope and indentation which share no tokens in common. Although this SCT may occasionally introduce semantic drift in imperative or effectful code, it ideally represents an alternate topological sort on the dataflow graph.
      \item The \lstinline|permuteArgument| SCT performs a Fisher-Yates shuffle on the arguments of a user-defined function of dyadic or higher-arity, representing an alternate parameter order of some function outside the JDK standard library.
      \item The \lstinline|generateDocument| SCT uses the code completion model under test to insert a comment delimeter, then samples natural language tokens autoregressively until a stopword is encountered or at most N tokens are produced.
    \end{enumerate}

    Idempotent SCTs (i.e., snippets which remain unchanged after the SCT is applied) are considered invalid and discarded. Strictly speaking, we cannot rule out the possibility that any of the aforementioned SCTs produce semantic variants due to the inherent complexity of source code analysis, however we have manually validated their admissibility for a large fraction of cases. A more principled macro system would help to alleviate these concerns, however a framework for rewriting partial Java code snippets is, to the best of our knowledge, presently unavailable.


    Our framework is capable of evaluating both code completion and document synthesis using the same approach. This can be done either inside the model's latent space using a sensitivity margin, or using some metric on the raw data. We decided to focus on the input domain directly and consider two metrics: ROUGE-synonym and average multimask accuracy using top-1 softmax output.

    To measure the effect size of each transformation in the case of code completion, we uniformly sample and mask ten individual tokens from both the original and transformed code snippet for evaluation. We then collect the model's top token predictions for each mask location, and measure the average completion accuracy on the original and transformed code snippet, recording the relative difference in accuracy before and after transformation.

    \begin{figure}[H]
      \centering
      \begin{lstlisting}[basicstyle=\scriptsize\ttfamily, language=kotlin,label={lst:example2}]
 //-------------------------------|-------------------------------
 // 1.a) Original method          |   1.a) Synonymous Variant
 //-------------------------------|-------------------------------
                                  |
    public void flush(int b) {    |   public void flush(int b) {
      buffer.write((byte) b);     |     cushion.write((byte) b);
      buffer.compact();           |     cushion.compact();
    }                             |   }
                                  |
 //-------------------------------|-------------------------------
 // 2.a) Multi-masked method      |   2.b) Multi-masked variant
 //-------------------------------|-------------------------------
                                  |
    public void <MASK>(int b) {   |   public void <MASK>(int b) {
      buffer.<MASK>((byte) b);    |     cushion.<MASK>((byte) b);
      <MASK>.compact();           |     <MASK>.compact();
    }                             |   }
                                  |
 //-------------------------------|-------------------------------
 // 3.a) Model predictions        |   2.b) Model predictions
 //-------------------------------|-------------------------------
                                  |
    public void output(int b) {   |   public void append(int b) {
      buffer.write((byte) b);     |     cushion.add((byte) b);
      buffer.compact();           |     cushion.compact();
    }                             |   }
      \end{lstlisting}
    \end{figure}

    The model correctly predicted $\frac{2}{3}$ masked tokens in the original snippet, and $\frac{1}{3}$ after renaming, so the relative completion accuracy is $\frac{\frac{2}{3} - \frac{1}{3}}{\frac{2}{3}} = \frac{1}{2}$.

    \begin{figure}[H]
      \centering
      \includegraphics[width=0.35\textwidth]{figs/dataflow.png}
      \caption{For each code snippet, we apply an SCT to generate a cosmetic variant, mask and predict some subsequence, and measure the code completion model's shift in accuracy on a downstream task of interest, e.g. source code or document completion.}
      \label{fig:dataflow}
    \end{figure}

    Similarly, in the case of document completion, we mask a naturally-occuring comment and autoregressively synthesize a new comment in its place, then compare the ROUGE-scores of synthetic documents before and after transformation. In the following example, we apply the \lstinline|renameTokens| SCT. We then mask the comment on line 3 and autoregressively sample tokens from the decoder to generate a synthetic two comments, first before and then after applying the SCT. Here is an example comment generated by our document synthesizer using GraphCodeBERT:

    \begin{lstlisting}[basicstyle=\scriptsize\ttfamily]
 //--------------------------------------------------------------
 // 1.) Original method with ground truth document
 //--------------------------------------------------------------

    public void testBuildSucceeds(String gradleVersion) {
        setup( gradleVersion );
        // ~Make sure the test build setup actually compiles~
        BuildResult buildResult = getRunner().build();
        assertCompileOutcome( buildResult, SUCCESS );
    }

 //--------------------------------------------------------------
 // 2.) Synthetic document before applying SCT
 //--------------------------------------------------------------

    public void testBuildSucceeds(String gradleVersion) {
        setup( gradleVersion );
        // **build the tests with gradletuce compiler**
        BuildResult buildResult = getRunner().build();
        assertCompileOutcome( buildResult, SUCCESS );
    }

 //--------------------------------------------------------------
 // 3.) Synthetic document after applying renameTokens SCT
 //--------------------------------------------------------------

    public void testBuildSucceeds(String **gradleAdaptation**) {
        setup( **gradleAdaptation** );
        // **build the actual code for test suite generation**
        BuildResult buildResult = getRunner().build();
        assertCompileOutcome( buildResult, SUCCESS );
    }
    \end{lstlisting}

    In the example shown above, we applied the \lstinline|renameTokens| SCT, which renamed the original parameter \lstinline|gradleVersion| to \lstinline|gradleAdaptation|. In the first snippet, we can see the original method with the ground-truth comment. In the second snippet we see the synthetic comment for the original method. In the third and final snippet, is the synthetic document after renaming \lstinline|gradleVersion| to \lstinline|gradleAdaptation|. After applying this SCT, GraphCodeBERT decided to change its comment slightly: this divergence is used to obtain a metric over ROUGE scores.

    Initially, we seeded the document completion using \texttt{//<MASK>} and applied a greedy autoregressive decoding strategy, recursively sampling the softmax top-1 token and subsequently discarding all malformed comments. This strategy turns out to have a very high rejection rate, due to its tendency to produce whitespace or unnatural language tokens (e.g., greedy decoding can lead to sequences like \texttt{// ///// //...}). A simple fix is to select the highest-scoring prediction with natural language characters. By conditioning on at least one alphabetic character per token, one obtains more coherent documentation and rejects fewer samples. One could imagine using a more sophisticated natural language filter, however we did not explore this in great depth.

    In the following section we report our results for each model, SCT and complexity.

    \pagebreak\section{Results}\label{sec:results}

    We evaluate three state-of-the-art pretrained models on two downstream tasks: code completion and document synthesis. While the sample sizes vary, we provide the same wall clock time (180 minutes) and hardware resources (NVIDIA Tesla V100) to each model. The number of code snippets they can evaluate in the allotted time may vary depending on the architecture, but in each case, the significant figures have mostly converged by this time. Complexity is measured using the procedure described in \S~\ref{sec:slicing}.

    Below, we report the average relative difference in completion accuracy after applying the SCT in the column to a code snippet whose Dyck-1 complexity is reported in the row heading, plus or minus the variance, with a sample size reported in parentheses.

      {\center

    CodeBERT
    \begin{table}[H]
      \tiny
      \begin{tabular}{r|cccc}
        Complexity          & \lstinline|renameTokens|        & \lstinline|swapMultilineNo|     & \lstinline|permuteArgument|     & \lstinline|addExtraLogging|     \\\hline\\
        10-20               & -0.13 ± 0.094 (42)  & 0.040 ± 0.329 (156) & 0.208 ± 0.348 (359) & 0.033 ± 0.082 (15)  \\
        20-30               & -0.26 ± 0.189 (112) & 0.137 ± 0.299 (312) & 0.116 ± 0.338 (542) & -0.01 ± 0.202 (82)  \\
        30-40               & -0.29 ± 0.224 (62)  & 0.098 ± 0.264 (163) & 0.185 ± 0.335 (329) & 0.081 ± 0.109 (73)  \\
        40-50               & -0.27 ± 0.222 (74)  & 0.142 ± 0.373 (138) & 0.092 ± 0.357 (232) & 0.043 ± 0.208 (82)  \\
        50-60               & -0.09 ± 0.295 (66)  & 0.041 ± 0.282 (130) & 0.120 ± 0.267 (335) & 0.014 ± 0.181 (136) \\
        60-70               & -0.21 ± 0.244 (60)  & 0.020 ± 0.280 (108) & 0.161 ± 0.252 (179) & -0.02 ± 0.211 (98)  \\
        70-80               & -0.11 ± 0.384 (24)  & 0.071 ± 0.343 (55)  & 0.081 ± 0.376 (79)  & -0.03 ± 0.356 (73)  \\
        80-90               & -0.12 ± 0.325 (42)  & 0.080 ± 0.363 (70)  & 0.035 ± 0.429 (97)  & -0.04 ± 0.350 (75)  \\
        90-100              & -0.04 ± 0.307 (37)  & 0.214 ± 0.291 (52)  & 0.218 ± 0.293 (70)  & 0.075 ± 0.226 (69)  \\
        100-110             & -0.07 ± 0.489 (23)  & 0.149 ± 0.345 (29)  & 0.037 ± 0.427 (44)  & 0.140 ± 0.467 (41)  \\
        110-120             & 0.092 ± 0.335 (18)  & -0.01 ± 0.473 (33)  & -0.04 ± 0.499 (43)  & -0.11 ± 0.236 (32)  \\
        120-130             & 0.076 ± 0.276 (15)  & 0.141 ± 0.235 (13)  & 0.150 ± 0.352 (21)  & 0.111 ± 0.355 (22)  \\
        130-140             & 0.129 ± 0.207 (12)  & 0.178 ± 0.340 (21)  & 0.123 ± 0.349 (27)  & 0.033 ± 0.488 (27)  \\
        140-150             & -0.14 ± 0.387 (9)   & 0.178 ± 0.414 (14)  & 0.107 ± 0.457 (17)  & 0.126 ± 0.542 (19)  \\
      \end{tabular}
    \end{table}


    GraphCodeBERT
    \begin{table}[H]
      \tiny
      \begin{tabular}{r|cccc}
        Complexity          & \lstinline|renameTokens|        & \lstinline|swapMultilineNo|     & \lstinline|permuteArgument|     & \lstinline|addExtraLogging|     \\\hline\\
        10-20               & -0.31 ± 0.204 (21)  & 0.201 ± 0.384 (114) & 0.137 ± 0.374 (276) & 0.166 ± 0.055 (6)   \\
        20-30               & -0.18 ± 0.155 (93)  & 0.130 ± 0.317 (247) & 0.034 ± 0.323 (438) & -0.03 ± 0.209 (71)  \\
        30-40               & -0.19 ± 0.233 (48)  & 0.174 ± 0.209 (149) & 0.198 ± 0.340 (288) & -0.04 ± 0.108 (64)  \\
        40-50               & -0.22 ± 0.256 (61)  & 0.093 ± 0.346 (94)  & 0.078 ± 0.346 (181) & -0.08 ± 0.282 (63)  \\
        50-60               & -0.26 ± 0.228 (59)  & 0.064 ± 0.259 (139) & 0.099 ± 0.280 (323) & -0.06 ± 0.185 (135) \\
        60-70               & -0.21 ± 0.207 (45)  & 0.058 ± 0.251 (85)  & 0.122 ± 0.249 (170) & -0.00 ± 0.224 (82)  \\
        70-80               & -0.39 ± 0.319 (17)  & 0.126 ± 0.417 (46)  & 0.072 ± 0.335 (69)  & -0.11 ± 0.315 (63)  \\
        80-90               & -0.00 ± 0.339 (37)  & -0.00 ± 0.294 (64)  & 0.056 ± 0.340 (85)  & -0.01 ± 0.295 (69)  \\
        90-100              & -0.13 ± 0.291 (29)  & 0.209 ± 0.386 (49)  & 0.035 ± 0.342 (67)  & 0.011 ± 0.254 (61)  \\
        100-110             & -0.18 ± 0.289 (17)  & 0.175 ± 0.377 (31)  & 0.005 ± 0.328 (39)  & 0.074 ± 0.381 (34)  \\
        110-120             & -0.01 ± 0.304 (14)  & 0.130 ± 0.454 (29)  & 0.288 ± 0.469 (34)  & -0.01 ± 0.281 (28)  \\
        120-130             & 0.022 ± 0.505 (10)  & -0.09 ± 0.408 (13)  & -0.13 ± 0.403 (19)  & -0.01 ± 0.367 (20)  \\
        130-140             & 0.033 ± 0.331 (11)  & 0.120 ± 0.406 (20)  & 0.032 ± 0.401 (22)  & 0.005 ± 0.530 (24)  \\
        140-150             & 0.583 ± 0.195 (7)   & -0.04 ± 0.361 (14)  & 0.274 ± 0.290 (17)  & 0.062 ± 0.315 (18)  \\
      \end{tabular}
    \end{table}

    RoBERTa
    \begin{table}[H]
      \tiny
      \begin{tabular}{r|cccc}
        Complexity          & \lstinline|renameTokens|        & \lstinline|swapMultilineNo|     & \lstinline|permuteArgument|     & \lstinline|addExtraLogging|     \\\hline\\
        10-20               & -0.42 ± 0.175 (122) & 0.296 ± 0.406 (277) & 0.387 ± 0.349 (704) & 0.0 ± 0.0 (12)      \\
        20-30               & -0.33 ± 0.172 (168) & 0.258 ± 0.338 (460) & 0.302 ± 0.288 (838) & -0.04 ± 0.145 (101) \\
        30-40               & -0.23 ± 0.188 (107) & 0.084 ± 0.261 (313) & 0.224 ± 0.311 (604) & 0.031 ± 0.172 (142) \\
        40-50               & -0.05 ± 0.237 (118) & 0.183 ± 0.291 (249) & 0.254 ± 0.268 (412) & -3.07 ± 0.098 (155) \\
        50-60               & -0.06 ± 0.239 (108) & 0.085 ± 0.253 (259) & 0.246 ± 0.242 (510) & -0.00 ± 0.138 (203) \\
        60-70               & -0.03 ± 0.196 (80)  & -4.31 ± 0.282 (171) & 0.174 ± 0.273 (291) & -0.02 ± 0.240 (144) \\
        70-80               & 0.124 ± 0.409 (35)  & 0.062 ± 0.253 (97)  & 0.174 ± 0.338 (132) & -0.01 ± 0.235 (107) \\
        80-90               & -0.06 ± 0.394 (43)  & 0.053 ± 0.350 (94)  & 0.225 ± 0.359 (132) & -0.00 ± 0.296 (103) \\
        90-100              & 0.118 ± 0.341 (47)  & 0.064 ± 0.347 (77)  & 0.294 ± 0.339 (95)  & 0.124 ± 0.309 (88)  \\
        100-110             & -0.11 ± 0.454 (32)  & -0.00 ± 0.475 (68)  & -0.00 ± 0.497 (81)  & -0.10 ± 0.411 (59)  \\
        110-120             & -0.16 ± 0.393 (32)  & 0.008 ± 0.498 (57)  & 0.199 ± 0.521 (63)  & -0.02 ± 0.527 (52)  \\
        120-130             & 0.132 ± 0.249 (20)  & 0.155 ± 0.591 (31)  & 0.291 ± 0.378 (41)  & 0.030 ± 0.496 (39)  \\
        130-140             & 0.111 ± 0.328 (18)  & 0.027 ± 0.519 (39)  & 0.256 ± 0.463 (46)  & 7.575 ± 0.494 (44)  \\
        140-150             & 0.265 ± 0.370 (23)  & 0.109 ± 0.575 (32)  & 0.357 ± 0.311 (33)  & 0.201 ± 0.500 (33)  \\
        150-160             & -0.04 ± 0.602 (12)  & 0.214 ± 0.407 (21)  & 0.064 ± 0.487 (26)  & 0.125 ± 0.512 (24)  \\
      \end{tabular}
    \end{table}
    }

    Sample size varies across SCTs because not all SCTs modify candidate snippets and we discard synthetic code snippets which are unchanged after transformation.

    As we can see, RoBERTa is considerably more sensitive to cosmetic variance than CodeBERT and GraphCodeBERT. In all cases, the  \lstinline|swapMultilineNo| and \lstinline|permuteArgument| SCTs exhibit a more detrimental effect than the other SCTs. Unexpectedly, it appears that token renaming tends to improve completion accuracy across all models on average.

    Below are the results for document synthesis, using the ROUGE-synonym metric. Like before, we report the average relative difference in ROUGE-synonym scores alongside their variance and sample size for each SCT, complexity bucket and model.

      {\center
    GraphCodeBERT
    \begin{table}[H]
      \tiny
      \begin{tabular}{l|cccc}
        Complexity          & renameTokens        & swapMultilineNo     & permuteArgument     & addExtraLogging     \\\hline\\
        40-50               & 6.365 ± 0.0 (1)     & 1.697 ± 0.0 (1)     & 8.575 ± 91.68 (2)   & NaN ± NaN (0)       \\
        50-60               & NaN ± NaN (0)       & NaN ± NaN (0)       & 0.030 ± 0.848 (5)   & -0.70 ± 0.0 (1)     \\
        60-70               & NaN ± NaN (0)       & -0.25 ± 0.281 (3)   & 3.922 ± 129.3 (8)   & -0.29 ± 0.036 (2)   \\
        70-80               & NaN ± NaN (0)       & -0.66 ± 0.0 (1)     & 0.947 ± 3.621 (6)   & 3.689 ± 17.37 (3)   \\
        80-90               & 5.833 ± 0.0 (1)     & -0.66 ± 0.0 (1)     & -0.02 ± 0.720 (7)   & -0.45 ± 0.595 (3)   \\
        90-100              & 5.417 ± 0.0 (1)     & 3.179 ± 22.20 (7)   & 3.746 ± 13.65 (12)  & 2.551 ± 0.0 (1)     \\
        100-110             & NaN ± NaN (0)       & 0.040 ± 0.909 (7)   & 1.156 ± 8.423 (13)  & -0.54 ± 0.539 (7)   \\
        110-120             & 1.323 ± 9.628 (4)   & 1.162 ± 10.31 (7)   & 2.232 ± 35.40 (8)   & -0.62 ± 0.429 (4)   \\
        120-130             & 0.363 ± 0.0 (1)     & 0.111 ± 1.120 (4)   & 0.105 ± 1.234 (5)   & -0.64 ± 0.028 (2)   \\
        130-140             & -1.0 ± 0.0 (1)      & 2.764 ± 34.12 (7)   & 1.446 ± 10.41 (11)  & 2.840 ± 18.54 (7)   \\
        140-150             & 1.039 ± 0.139 (2)   & -0.35 ± 0.106 (3)   & 0.668 ± 3.425 (7)   & 2.209 ± 27.07 (10)
      \end{tabular}
    \end{table}

    CodeBERT

    \begin{table}[H]
      \tiny
      \begin{tabular}{l|cccc}
        Complexity          & renameTokens        & swapMultilineNo     & permuteArgument     & addExtraLogging     \\\hline\\
        40-50               & NaN ± NaN (0)       & NaN ± NaN (0)       & -0.20 ± 0.0 (1)     & -0.75 ± 0.0 (1)     \\
        60-70               & NaN ± NaN (0)       & NaN ± NaN (0)       & 25.33 ± 1186. (3)   & NaN ± NaN (0)       \\
        70-80               & -1.0 ± 0.0 (1)      & 0.147 ± 0.001 (2)   & 6.757 ± 213.8 (5)   & NaN ± NaN (0)       \\
        80-90               & 0.178 ± 0.023 (2)   & 0.65 ± 3.261 (3)    & -0.20 ± 0.016 (2)   & -1.0 ± 0.0 (1)      \\
        90-100              & -0.85 ± 0.0 (1)     & NaN ± NaN (0)       & NaN ± NaN (0)       & NaN ± NaN (0)       \\
        100-110             & 1.999 ± 0.0 (1)     & 0.111 ± 2.469 (3)   & -0.73 ± 0.142 (3)   & NaN ± NaN (0)       \\
        110-120             & 0.071 ± 0.0 (1)     & -0.56 ± 0.068 (2)   & 2.345 ± 28.16 (6)   & -0.95 ± 0.0 (1)     \\
        120-130             & -0.81 ± 0.0 (1)     & 0.307 ± 0.036 (2)   & 1.549 ± 12.76 (5)   & NaN ± NaN (0)       \\
        130-140             & -0.33 ± 0.005 (2)   & -0.02 ± 0.137 (6)   & 2.131 ± 8.023 (9)   & 20.66 ± 0.0 (1)     \\
        140-150             & -1.0 ± 0.0 (1)      & -0.24 ± 0.304 (3)   & 0.239 ± 1.016 (6)   & -0.51 ± 0.0 (1)
      \end{tabular}
    \end{table}

    RoBERTa

    \begin{table}[H]
      \tiny
      \begin{tabular}{l|cccc}
        Complexity          & renameTokens        & swapMultilineNo     & permuteArgument     & addExtraLogging     \\\hline\\
        30-40               & NaN ± NaN (0)       & NaN ± NaN (0)       & 3.142 ± 14.87 (2)   & 1.4 ± 0.0 (1)       \\
        40-50               & -0.69 ± 0.037 (2)   & 0.644 ± 19.80 (13)  & 0.350 ± 12.70 (39)  & -0.53 ± 0.864 (9)   \\
        50-60               & NaN ± NaN (0)       & 1.653 ± 9.808 (14)  & 1.671 ± 26.44 (85)  & 3.194 ± 75.10 (8)   \\
        60-70               & NaN ± NaN (0)       & 0.186 ± 1.067 (15)  & 2.107 ± 19.51 (82)  & 0.218 ± 4.887 (8)   \\
        70-80               & NaN ± NaN (0)       & 9.666 ± 183.9 (8)   & 2.515 ± 56.38 (48)  & 0.200 ± 3.070 (10)  \\
        80-90               & 0.210 ± 1.319 (4)   & 3.016 ± 33.88 (15)  & 1.774 ± 46.96 (55)  & 4.167 ± 49.65 (14)  \\
        90-100              & -0.35 ± 0.075 (3)   & 0.520 ± 1.556 (21)  & 2.435 ± 39.08 (54)  & 0.407 ± 2.483 (18)  \\
        100-110             & 2.645 ± 13.31 (4)   & 0.613 ± 5.410 (19)  & 2.900 ± 96.60 (57)  & 1.348 ± 14.02 (27)  \\
        110-120             & 0.077 ± 0.915 (6)   & 2.712 ± 79.64 (17)  & 1.945 ± 48.44 (46)  & 0.482 ± 2.780 (16)  \\
        120-130             & 4.345 ± 68.67 (7)   & -0.01 ± 0.648 (17)  & 1.023 ± 10.94 (46)  & 0.169 ± 2.044 (26)  \\
        130-140             & 1.633 ± 23.58 (8)   & 1.375 ± 26.31 (18)  & 3.184 ± 90.34 (57)  & 0.469 ± 2.741 (25)  \\
        140-150             & 3.250 ± 11.12 (6)   & 1.104 ± 4.900 (22)  & 4.442 ± 237.8 (41)  & 0.685 ± 5.981 (27)
      \end{tabular}
    \end{table}
    }

    \section{Discussion}

    We can also reproduce the relative rankings of the models as reported in relevant literature: RoBERTa $<<$ CodeBERT $<$ GraphCodeBERT.

    We observe a clear trend over method complexity: SCTs in low complexity code have a larger effect than similar transformations in high-complexity code. We hypothesize this trend can be explained by the fact that rewriting can have comparatively large effect when the surrounding code snippet is tiny and therefor contains less contextual information.

    If we examine the source code snippets, we notice that renaming can have a significant effect on document synthesis. Examining the synthesized documents, we can see that the model frequently copies tokens from the source code, so renaming can degrade document quality. Similarly swapping multiline statements also reduces document quality.\ldots

    \section{Conclusion}\label{sec:conclusion}

    The work described herein is primarily an empirical study, but also represents a framework and a set of methodological practices which may be used to evaluate future code-based neural language models, offering several advantages from a software engineering standpoint. Due to its functional implementation, it is efficient, parallelizable and highly modular. Further details about the architecture and its benefits can be found in \S~\ref{sec:architecture}.

    Despite its simplicity, the Regex-based SCT approach has some shortcomings. Although regular expressions are easy to implement and do support rudimentary transformations, they are a crude way to manipulate source code. In order to generate semantically valid transformations, one must really use full-fledged term rewriting system, such as higher-order abstract syntax or some kind of parser-combinator. Several options were evaluated, including OpenRewrite, TXL, Refal et al., but their features were ultimately found wanting (e.g., poor error recovery) and the complexity of using them (e.g., parsing, API integration) proved too burdensome.

    We have identified three rewriting categories, corresponding to possible types of source code transformations:

    \begin{enumerate}
      \item Syntactic - can the LM detect syntactically invalid SCTs? (e.g., syntax corruption, imbalanced parenthesis)
      \item Structural - can the LM detect syntactically valid, but semantically unsound PTs? (e.g., constant modification, operator substitution and order of operations)
      \item Cosmetic - can the LM detect purely cosmetic SCTs? (e.g., variable renaming, documentation)
    \end{enumerate}

    The present work falls into the lattermost category, however other types of SCTs may be considered in future work. We currently only use average mutli-mask accuracy and ROUGE score, although we hope to compare various other metrics such as mean average precision (MAP), mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG) in the future.

    One intriguing avenue for future work would be to consider combinations of source code transformations. This would vastly expand the cardinality of the validation set, enabling us to access a much larger space of ``possible worlds'' albeit potentially at the risk of lower semantic admissibility as arbitrary combinations of SCTs can quickly produce invalid code. This would be interesting engineering problem and possible extension to this work.

    Finally, we could use the code completion model itself to generate code for testing the same model. We have implemented this to a limited degree in the \lstinline|addExtraLogging| SCT, where the model predicts a single token to log, and the \lstinline|generateDocument| SCT, where the model completes a document, however this could be a useful way to generate extra training data. This would require careful postprocessing.

    Neural language models hold much promise for improved code completion, however complacency can lead to increased reviewer burden or more serious technical debt if widely adopted. While trade secrecy may prevent third-party inspection of pretrained models, users would still like some assurance of the model's robustness to naturally-occurring variance. Our work helps to address this by generating plausible cosmetic variants and measuring end-to-end robustness of the neural language model.
    \pagebreak\bibliography{main}
    \bibliographystyle{plain}
    \appendix

    \section{Slicing procedure}\label{sec:slicing}

    We describe below a simple heuristic for extracting method slices in well-formed source code using a Dyck counter.~\footnote{\url{https://en.wikipedia.org/wiki/Dyck\_language}} A common coding convention is to prefix functions with a keyword, followed by a group of balanced brackets and one or more blank lines. While imperfect, we observe this pattern can be used to slice methods in a variety of languages in practice. A Kotlin implementation is given below, which will output the following source code when run on itself:

    \vspace{11pt}

    \begin{lstlisting}[basicstyle=\scriptsize\ttfamily, language=kotlin,label={lst:example4}]
fun String.sliceIntoMethods(kwds: Set<String> = setOf("fun ")) =
  lines().fold(-1 to List<String>(0)) { (dyckCtr, methods), ln ->
    if (dyckCtr < 0 && kwds.any { it in ln }) {
      ln.countBalancedBrackets() to (methods + ln)
    } else if (dyckCtr == 0) {
      if (ln.isBlank()) -1 to methods else 0 to methods.put(ln)
    } else if (dyckCtr > 0) {
      dyckCtr + ln.countBalancedBrackets() to methods.put(ln)
    } else -1 to methods
  }.second

fun List<String>.put(s: String) = dropLast(1) + (last() + "\n$s")

fun String.countBalancedBrackets() = fold(0) { sum, char ->
  val (lbs, rbs) = setOf('(', '{', '[') to setOf(')', '}', ']')
  if (char in lbs) sum + 1 else if (char in rbs) sum - 1 else sum
}

fun main(args: Array<String>) =
  println(args[0].sliceIntoMethods().joinToString("\n\n"))
    \end{lstlisting}

    Using this approach, we can approximate the bracket complexity for each code snippet and slice methods horizontally. More elaborate dataflow-based slicing procedures require parsing.

    \section{Architectural details}\label{sec:architecture}

    Our experimental architecture is to our knowledge, unique, and merits some discussion. The entire pipeline from data mining to preprocessing, evaluation and table generation is implemented as a pure functional program in the point-free style, enabling straightforward parallelization.

    We can view the tables in this paper as 2D-slices of a rank-n tensor representing an n-dimensional hyperdistribution formed by the Cartesian product of all variables under investigation (e.g. code complexity, metric, task, model). During evaluation, we sample the independent variables uniformly to ensure its entries are evenly populated. Results are continuously delivered to the user, who may preview 2D marginals for any pair and watch the error bounds grow tighter as additional samples are drawn.

    This kind of feature is useful when running on preemptible infrastructure and can be massively parallelized to increase the experiment's statistical power or explore larger subspaces of the experimental design space.

    \section{Analysis of internal structure}\label{sec:probe_internals}

    In the figure below, we compute the distance between pairs of random code snippets in CodeBERT latent space and find a positive correlation with various string distance metrics.

    \begin{figure}[H]
      \centering
      \resizebox{0.45\textwidth}{!}{%
        \begin{tikzpicture}
          \begin{axis}[width=0.29\textwidth, height=0.3\textwidth, ymax=7, xlabel=Levenshtein, ylabel=Euclidean]
            \addplot table [mark=none,x=strdist, y=embdist, variable=var, col sep=comma] {data/levenshtein.csv};

            \addplot [smooth, name path=upper,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}+\thisrow{var}, col sep=comma] {data/levenshtein.csv};
            \addplot [smooth, name path=lower,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}-\thisrow{var}, col sep=comma] {data/levenshtein.csv};
            \addplot [fill=red!10] fill between[of=upper and lower];
          \end{axis}
        \end{tikzpicture}
        \begin{tikzpicture}
          \begin{axis}[width=0.29\textwidth, height=0.3\textwidth, ymax=7, xlabel=Damerau]
            \addplot table [mark=none,x=strdist, y=embdist, variable=var, col sep=comma] {data/damerau.csv};

            \addplot [smooth, name path=upper,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}+\thisrow{var}, col sep=comma] {data/damerau.csv};
            \addplot [smooth, name path=lower,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}-\thisrow{var}, col sep=comma] {data/damerau.csv};
            \addplot [fill=red!10] fill between[of=upper and lower];
          \end{axis}
        \end{tikzpicture}
        \begin{tikzpicture}
          \begin{axis}[width=0.29\textwidth, height=0.3\textwidth, ymax=7, xlabel=MetricLCS]
            \addplot table [mark=none,x=strdist, y=embdist, variable=var, col sep=comma] {data/metriclcs.csv};

            \addplot [smooth, name path=upper,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}+\thisrow{var}, col sep=comma] {data/metriclcs.csv};
            \addplot [smooth, name path=lower,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}-\thisrow{var}, col sep=comma] {data/metriclcs.csv};
            \addplot [fill=red!10] fill between[of=upper and lower];
          \end{axis}
        \end{tikzpicture}
        \begin{tikzpicture}
          \begin{axis}[width=0.29\textwidth, height=0.3\textwidth, ymax=7, xlabel=Jaccard]
            \addplot table [mark=none,x=strdist, y=embdist, variable=var, col sep=comma] {data/jaccard.csv};

            \addplot [smooth, name path=upper,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}+\thisrow{var}, col sep=comma] {data/jaccard.csv};
            \addplot [smooth, name path=lower,draw=none] table[x=strdist, y=embdist,variable=var, y expr=\thisrow{embdist}-\thisrow{var}, col sep=comma] {data/jaccard.csv};
            \addplot [fill=red!10] fill between[of=upper and lower];
          \end{axis}
        \end{tikzpicture}
      }
      \caption{CodeBERT latent space vs. string edit distance.}
      \label{fig:lev_vs_euclid}
    \end{figure}

%  \begin{figure}[H]
%    \centering
%    \includegraphics[width=0.3\textwidth]{figs/rename_tokens}
%    \caption{TSNE-embedded code snippets before (green) and after (orange) synonym renaming was applied.}
%    \label{fig:embedding}
%  \end{figure}

    We fetch a dataset of Java and Kotlin repositories sampled repositories on GitHub, containing a mixture of filetypes representing source code and natural language artifacts.
%
    From each repository, we index all substrings of every line in every file using a variable height radix tree producing a multimap of $\texttt{String}$ queries to file-offset pairs. We also encode CodeBERT~\citep{feng2020codebert} sequence embeddings to substrings $\texttt{knnIndex: Vector -> Location<F, O>}$ using a Hierarchial Navigavble Small World Graph~\citep{malkov2018efficient} (HNSWG).


    \begin{figure}[H]
      \centering
      \includegraphics[width=0.4\textwidth]{figs/latent_kg}
      \caption{To compute our query neighborhood, we traverse the HNSWG up to max depth $d$, i.e. $d=1$ fetches the neighbors, $d=2$ fetches the neighbors-of-neighbors, and $d=3$ fetches the neighbors-of-neighbors-of-neighbors.}
      \label{fig:de2kg}
    \end{figure}

    \begin{figure}[H]
      \centering
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/query4}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/query5}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/query6}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/query7}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/context4}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/context5}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/context6}}
      \resizebox{.1\textwidth}{!}{\includegraphics{../data/context7}}
      \caption{Virtual knowledge graph constructed by searching Euclidean nearest-neighbors in latent space. Edges represent the k-nearest neighbors, up to depth-n (i.e., k=5, n=3).}
      \label{fig:graphs}
    \end{figure}


  \end{document}
  \endinput


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{figs/dataflow.png}
    \caption{For each code snippet, we apply an SCT to generate a cosmetic variant, mask and predict some subsequence, and measure the code completion model's shift in accuracy on a downstream task of interest, e.g. source code or document completion.}
    \label{fig:dataflow}
  \end{figure}

\end{document}