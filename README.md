# CSTK: Code Search Toolkit

Tools and experiments for code search. Broadly, we consider code synthesis as a search problem: [programming](https://breandan.net/public/programming_with_intelligent_machines.html) is like a kind of biased random walk through edit space. Program synthesis then, can be viewed as is a goal-directed Markov decision process which takes a specification, and applies a sequence of source code transformations to evolve the code towards some specification (e.g. test- or document- driven development). This repository provides tools for evaluating state-of-the-art neural code synthesizers by exploring various tasks, from natural language and code search and completion, optimal transport, to various couplings between code and documentation.

* Probing tools for pretrained neural language models
* Autoregressive code and document completion with masked LMs
* [Full-factorial experiments](https://en.wikipedia.org/wiki/Factorial_experiment) on source code
* Indices for keyword and vector embedding
* Learning to search & grammar induction
  * Passive DFA learning from membership
  * Keyword/BoW-based query synthesis
* Semantic graph construction
  * Keyword-matching edge construction
  * Proximity-based graph embedding
* Vector embeddings for code
  * Parsing and whole-AST GNN embeddings
  * Transformer embeddings of source code snippets
  * [t-SNE visualization](src/main/kotlin/edu/mcgill/cstk/experiments/VizCodeEmbeddings.kt) of code embeddings
* [Persistent homology](#persistent-homology) of source code embeddings
* Metrics for string, vector and distribution matching
  * Kantorovich metric on code embeddings
  * Various string distance metrics
  * Code-snippet normal form distance
  * Ranking metrics: NDCG, MAP@K, MRR
  * [Comparison of nearest-neighbors](#nearest-neighbor-search)
* Tools for mining software repositories
  * Supports Google Code, Gitlab, and self-hosted Git instances
  * Deduplication with GitHub to avoid dataset biases
* [Probabilistic code synthesis with Markov tensors](#probabilistic-code-synthesis)
* Synthetic source code transformations
  * Synonym variable renaming
  * Dead code introduction
  * Loop bounds alteration
  * Argument order swapping
  * Line order swapping

Code and documentation are complementary and synergistic datatypes. A good programmer should be able to read and write both. We expect a neural programmer to attain fluency in both human and programming languages and evaluate the extent to which SOTA neural language models have mastered this ability. This indicates they have some understanding of intent.

We try our best to take an empirical approach. All experiments are conducted on a relatively diverse sampling of repositories from GitHub containing a mixture of source code and documentation. In those experiments, we use code completion, code search and other downstream tasks to compare the accuracy of pretrained models in constructed scenarios.

# Usage

## Setup

First clone this repo and initialize the submodule:

```bash
git clone git@github.com:breandan/cstk.git && \
cd cstk && \
git submodule update --init --recursive --remote
```

The following instructions assume you are running experiments on Compute Canada such as Narval or a similar cluster. Create a virtual environment and install the following dependencies:

```bash
module load python/3.8 && \
python3 -m venv . && \
source bin/activate && \
pip install torch==1.5.1 -f https://download.pytorch.org/whl/torch_stable.html && \
pip install transformers
```

Prefetch the models you wish to evaluate from the login node -- this will require internet access. Each model must provide a [`fill-mask` pipeline](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.FillMaskPipeline) (see here for a list of [compatible models](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads&search=code)).

```bash
python scripts/embedding_server.py --model microsoft/codebert-base microsoft/graphcodebert-base dbernsohn/roberta-java huggingface/CodeBERTa-small-v1
```

Once all models have been downloaded, kill it with <kbd>Ctrl</kbd>+<kbd>C</kbd> (this step should only need to be run once). Confirm that `~/.cache/huggingface/transformers` is not empty.

Then, make sure the project builds correctly on a login node and download the dataset. This make take a few minutes the first time it is run:

```bash
# Load Java through CCEnv when running on Niagara:
# module load CCEnv StdEnv java/17.0.2 && \
module load java/17.0.2 && \
./gradlew build && \
./gradlew cloneRepos
```

To run an experiment interactively, request a GPU instance like so:

```bash
salloc --time 3:0:0 --account=[YOUR_ADVISOR] --gres=gpu:a100:1 --mem=40G
```

Compute nodes have no internet, so future commands will need to occur offline.

```sh
# Default CUDA version may not work, use older version
export LD_LIBRARY_PATH=/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/cudacore/10.2.89/targets/x86_64-linux/lib/
# Disable ðŸ¤— from phoning home on a Compute node
export TRANSFORMERS_OFFLINE=1
module load python/3.8
# Load Java through CCEnv when running on Niagara:
# module load CCEnv StdEnv java/17.0.2 && \
module load java/17.0.2
source bin/activate
# Use --offline for all Gradle commands on Compute nodes
./gradlew --offline [completeCode] [completeDoc] [varMisuse]
```

Once you have confirmed the experiment runs smoothly and are ready to submit a longer job, edit [`niagara_submit.sh`](niagara_submit.sh) and run one of the following commands to submit it to Slurm, depending on the Compute Canada cluster you want to run it on (Narval has 64 cores, Niagara has 40):

```bash
sbatch niagara_submit.sh
sbatch narval_submit.sh
```

## Experiments

Experiments are mostly self-contained. Each Gradle task corresponds to a single experiment. They have been tested on JDK 21.

### Syntax repair

*The primary artifact and setup instructions for the syntax repair experiments are available on [Zenodo](https://zenodo.org/records/13989124).*

Other instructions for fetching the raw datasets and running other experiments can be found below.

First, fetch the [Break-It-Fix-It](https://arxiv.org/pdf/2106.06600.pdf) and [StackOverflow](https://arxiv.org/pdf/1907.07803.pdf) datasets as follows:

```bash
export DATASET_DIR=src/main/resources/datasets/python && \
wget -O $DATASET_DIR/stack_overflow.zip https://figshare.com/ndownloader/articles/8244686/versions/1 && \
mkdir -p $DATASET_DIR/stack_overflow && \
unzip $DATASET_DIR/stack_overflow.zip -d $DATASET_DIR/stack_overflow && \
wget -O $DATASET_DIR/bifi.zip https://nlp.stanford.edu/projects/myasu/BIFI/data_minimal.zip && \
mkdir -p $DATASET_DIR/bifi && \
unzip $DATASET_DIR/bifi.zip -d $DATASET_DIR/bifi
```

The following commands will run the one of the [syntax repair experiments](src/main/kotlin/edu/mcgill/cstk/experiments/repair/):

```bash
./gradlew contextualRepair
./gradlew pythonSnippetRepair
./gradlew pythonStatementRepair
./gradlew kotlinSemanticRepair
./gradlew kotlinStatementRepair
```

Most of these experiments leverage parallelism, so the more CPUs are available, the better. We use AWS EC2 `c7a.24xlarge` instances with 96 vCPUs and 192 GB of RAM.

### Mining software repositories

Tokens for accessing the [GitHub](https://docs.github.com/en/rest/reference/search) and [GitLab](https://docs.github.com/en/rest/reference/search) developer APIs should be placed in the `.ghtoken` and `.gltoken` files, respectively.

The following command is optional and will sample some repositories from [GitHub](github.txt), [GitLab](gitlab.txt), [Google Code](gcode.txt). To change the repository selection criteria, edit [`SampleRepos.kt`](src/main/kotlin/edu/mcgill/cstk/crawler/SampleRepos.kt):

```bash
./gradlew sampleRepos
```

Those repositories may be cloned for evaluation. The following command will download Git repos into the `data` directory by default. To change the defaults, edit [`CloneRepos.kt`](src/main/kotlin/edu/mcgill/cstk/crawler/CloneRepos.kt):

```bash
./gradlew cloneRepos
```

### Masked code completion

The following will run the [`CodeCompletion.kt`](src/main/kotlin/edu/mcgill/cstk/experiments/CodeCompletion.kt) demo:

```bash
./gradlew completeCode
```

We use this task to evaluate the impact of source code transformation. If the relative completion accuracy drops after a SCT has been applied, this indicates the model is sensitive to noise.

### Document synthesis

The following will run the [`DocCompletion.kt`](src/main/kotlin/edu/mcgill/cstk/experiments/DocCompletion.kt) demo:

```bash
./gradlew completeDoc
```

For example, here are some [synthetic documents produced by GraphCodeBERT](/latex/notes/all_synthetic_docs.pdf) using greedy autoregressive decoding with a natural language filter.

### Persistent homology

It is possible to visualize persistent homology. To construct a ÄŒech complex on a set of source code snippets run:

```
./gradlew nearestNeighbors
```

This will embed the snippets and construct edges between the nearest neighbors. It's a nice way to visualize code:

<details>

| Matrix | Graph |
---------|-------|
| <img src="latex/data/context4.png" height="200"/> | <img src="latex/data/query4.png" height="200"/>
| <img src="latex/data/context5.png" height="200"/> | <img src="latex/data/query5.png" height="200"/>
| <img src="latex/data/context6.png" height="200"/> | <img src="latex/data/query6.png" height="200"/>

</details>

### Source code transformations

CSTK supports a number of source code transformations for studying the effect on neural language models. Some examples are given below.

#### Synonym renaming

Synonym renaming is provided by [extJWNL](https://github.com/extjwnl/extjwnl). Run the following command:

```bash
./gradlew synonymize
```

<details>

Left column is the original, right column is synonymized:

```
fun VecIndex.knn(v: DoubleArray, i: Int, exact: Boolean = false) =    |    fun VecIndex.knn(v: DoubleArray, i: Int, involve: Boolean = false) =
  if(exact) exactKNNSearch(v, i + 10)                                 |      if(involve) involveKNNSearch(v, i + 10)
  else findNearest(v, i + 10)                                         |      else findNearest(v, i + 10)
    .filter { !it.item().embedding.contentEquals(v) }                 |        .filter { !it.item().embedding.contentEquals(v) }
    .distinctBy { it.item().toString() }.take(i)                      |        .distinctBy { it.item().toString() }.take(i)
============================================================================================================================================
fun VecIndex.exactKNNSearch(vq: DoubleArray, nearestNeighbors: Int) = |    fun VecIndex.exactKNNSearch(vq: DoubleArray, nearestEdge: Int) =
  asExactIndex().findNearest(vq, nearestNeighbors)                    |      asExactIndex().findNearest(vq, nearestEdge)
============================================================================================================================================
  override fun vector(): DoubleArray = embedding                      |      override fun variable(): DoubleArray = embedding
============================================================================================================================================
  override fun dimensions(): Int = embedding.size                     |      override fun mark(): Int = embedding.size
============================================================================================================================================
  override fun toString() = loc.getContext(0)                         |      override fun toWithdraw() = loc.getContext(0)
}                                                                     |    }
============================================================================================================================================
fun main() {                                                          |    fun main() {
  buildOrLoadVecIndex()                                               |      baseOrDepositVecFurnish()
}                                                                     |    }
============================================================================================================================================
fun buildOrLoadKWIndex(                                               |    fun buildOrLoadKWIndex(
  indexFile: File = File(DEFAULT_KNNINDEX_FILENAME),                  |      regulateIncriminate: File = File(DEFAULT_KNNINDEX_FILENAME),
  rootDir: URI = TEST_DIR                                             |      rootDir: URI = TEST_DIR
): KWIndex =                                                          |    ): KWIndex =
  if (!indexFile.exists())                                            |      if (!regulateIncriminate.exists())
    rebuildKWIndex(rootDir).apply { serializeTo(indexFile) }          |        rebuildKWIndex(rootDir).apply { serializeTo(regulateIncriminate) }
  else indexFile.deserializeFrom()                                    |      else regulateIncriminate.deserializeFrom()
============================================================================================================================================
fun main() {                                                          |    fun main() {
  buildOrLoadKWIndex(                                                 |      intensifyOrConcernKWFact(
    indexFile = File(DEFAULT_KWINDEX_FILENAME),                       |        indexFile = File(DEFAULT_KWINDEX_FILENAME),
    rootDir = File("data").toURI()                                    |        rootDir = File("data").toURI()
  )                                                                   |      )
}                                                                     |    }
============================================================================================================================================
fun String.shuffleLines() = lines().shuffled().joinToString("\n")     |    fun String.walkDepression() = lines().shuffled().joinToString("\n")
============================================================================================================================================
fun String.swapPlusMinus() =                                          |    fun String.goQualityMinus() =
  map { if (it == '+') '-' else it }.joinToString("")                 |      map { if (it == '+') '-' else it }.joinToString("")
============================================================================================================================================
fun String.fuzzLoopBoundaries(): String =                             |    fun String.fuzzLoopBoundaries(): String =
  replace(Regex("(for|while)(.*)([0-9]+)(.*)")) { match ->            |      replace(Regex("(for|while)(.*)([0-9]+)(.*)")) { change ->
    match.groupValues.let { it[1] + it[2] +                           |        change.groupValues.let { it[1] + it[2] +
      (it[3].toInt() + (1..3).random()) + it[4] }                     |          (it[3].toInt() + (1..3).random()) + it[4] }
  }                                                                   |      }
============================================================================================================================================
fun String.swapMultilineNoDeps(): String =                            |    fun String.swapMultilineNoDeps(): String =
  lines().chunked(2).map { lines ->                                   |      reenforce().chunked(2).map { reenforce ->
    if (lines.size != 2) return@map lines                             |        if (reenforce.size != 2) return@map reenforce
    val (a, b) = lines.first() to lines.last()                        |        val (a, b) = reenforce.first() to reenforce.last()
    // Same indentation                                               |        // Same indentation
    if (a.trim().length - a.length != b.trim().length - b.length)     |        if (a.trim().length - a.length != b.trim().length - b.length)
      return@map listOf(a, b)                                         |          return@map listOf(a, b)
============================================================================================================================================
fun String.addDeadCode(): String =                                    |    fun String.reckonDeadLabel(): String =
  lines().joinToString("\n") {                                        |      lines().joinToString("\n") {
    if (Math.random() < 0.3) "$it; int deadCode = 2;" else it         |        if (Math.random() < 0.3) "$it; int deadCode = 2;" else it
  }                                                                   |      }
============================================================================================================================================
fun main() = TrainSeq2Seq.runExample()                                |    fun main() = TrainSeq2Seq.contendRepresentation()
============================================================================================================================================
  override fun getData(manager: NDManager): Iterable<Batch> =         |      override fun buyData(manager: NDManager): Iterable<Batch> =
    object: Iterable<Batch>, Iterator<Batch> {                        |        object: Iterable<Batch>, Iterator<Batch> {
      var maskedInstances: List<MaskedInstance> = createEpochData()   |          var maskedInstances: List<MaskedInstance> = createEpochData()
      var idx: Int = batchSize                                        |          var idx: Int = batchSize
============================================================================================================================================
      override fun hasNext(): Boolean = idx < maskedInstances.size    |          override fun bangNext(): Boolean = idx < maskedInstances.size
============================================================================================================================================
  override fun prepare(progress: Progress?) {                         |      override fun prepare(progress: Progress?) {
    // get all applicable files                                       |        // get all applicable files
    parsedFiles = TEST_DIR.allFilesRecursively(FILE_EXT)              |        analyzeAccuse = TEST_DIR.allFilesRecursively(FILE_EXT)
      .map { it.toPath() }                                            |          .map { it.toPath() }
      // read & tokenize them                                         |          // read & tokenize them
      .map { parseFile(it) }                                          |          .map { parseFile(it) }
    // determine dictionary                                           |        // determine dictionary
    dictionary = buildDictionary(countTokens(parsedFiles))            |        dictionary = buildDictionary(countTokens(analyzeAccuse))
  }                                                                   |      }
============================================================================================================================================
  fun getDictionarySize(): Int = dictionary!!.tokens.size             |      fun channeliseDictionaryFiller(): Int = dictionary!!.tokens.size
============================================================================================================================================
    operator fun get(id: Int): String =                               |        operator fun get(id: Int): String =
      if (id >= 0 && id < tokens.size) tokens[id] else UNK            |          if (id >= 0 && id < sign.size) sign[id] else UNK
============================================================================================================================================
    operator fun get(token: String): Int =                            |        operator fun get(sign: String): Int =
      tokenToId.getOrDefault(token, UNK_ID)                           |          signToId.getOrDefault(sign, UNK_ID)
============================================================================================================================================
    fun toTokens(ids: List<Int>): List<String> = ids.map { this[it] } |        fun toSymbol(ids: List<Int>): List<String> = ids.map { this[it] }
============================================================================================================================================
    fun getRandomToken(rand: Random?): String =                       |        fun getRandomToken(rand: Random?): String =
      tokens[rand!!.nextInt(tokens.size)]                             |          disk[rand!!.nextInt(disk.size)]
============================================================================================================================================
    private fun batchFromList(                                        |        private fun batchFromList(
      ndManager: NDManager,                                           |          metalTrainer: NDManager,
      batchData: List<IntArray>                                       |          batchData: List<IntArray>
    ) = ndManager.create(batchData.toTypedArray())                    |        ) = metalTrainer.create(batchData.toTypedArray())
============================================================================================================================================
    private fun batchFromList(                                        |        private fun assemblageFromEnumerate(
      ndManager: NDManager,                                           |          ndManager: NDManager,
      instances: List<MaskedInstance>,                                |          instances: List<MaskedInstance>,
      f: (MaskedInstance) -> IntArray                                 |          f: (MaskedInstance) -> IntArray
    ): NDArray = batchFromList(ndManager, instances.map { f(it) })    |        ): NDArray = assemblageFromEnumerate(ndManager, instances.map { f(it) })
============================================================================================================================================
fun List<Double>.variance() =                                         |    fun List<Double>.variance() =
  average().let { mean -> map { (it - mean).pow(2) } }.average()      |      cypher().let { mean -> map { (it - mean).pow(2) } }.cypher()
============================================================================================================================================
fun euclidDist(f1: DoubleArray, f2: DoubleArray) =                    |    fun geometerDist(f1: DoubleArray, f2: DoubleArray) =
  sqrt(f1.zip(f2) { a, b -> (a - b).pow(2) }.sum())                   |      sqrt(f1.zip(f2) { a, b -> (a - b).pow(2) }.sum())
============================================================================================================================================
fun Array<DoubleArray>.average(): DoubleArray =                       |    fun Array<DoubleArray>.average(): DoubleArray =
  fold(DoubleArray(first().size)) { a, b ->                           |      fold(DoubleArray(first().size)) { a, b ->
    a.zip(b).map { (i, j) -> i + j }.toDoubleArray()                  |        a.zip(b).map { (i, j) -> i + j }.toBidVesture()
  }.map { it / size }.toDoubleArray()                                 |      }.map { it / size }.toBidVesture()
============================================================================================================================================
  override fun distance(u: DoubleArray, v: DoubleArray) =             |      override fun distance(u: DoubleArray, v: DoubleArray) =
    kantorovich(arrayOf(u), arrayOf(v))                               |        kantorovich(standOf(u), standOf(v))
}                                                                     |    }
============================================================================================================================================
fun main() {                                                          |    fun main() {
  val (a, b) = Pair(randomMatrix(400, 768), randomMatrix(400, 768))   |      val (a, b) = Pair(randomArray(400, 768), randomArray(400, 768))
  println(measureTime { println(kantorovich(a, b)) })                 |      println(measureTime { println(kantorovich(a, b)) })
}                                                                     |    }
============================================================================================================================================
    override fun processInput(                                        |        override fun processInput(
      ctx: TranslatorContext,                                         |          ctx: TranslatorContext,
      inputs: Array<String>                                           |          infix: Array<String>
    ): NDList = NDList(                                               |        ): NDList = NDList(
      NDArrays.stack(                                                 |          NDArrays.stack(
        NDList(inputs.map { ctx.ndManager.create(it) })               |            NDList(infix.map { ctx.ndManager.create(it) })
      )                                                               |          )
    )                                                                 |        )
============================================================================================================================================
    override fun getBatchifier(): Batchifier? = null                  |        override fun channelizeBatchifier(): Batchifier? = null
  }                                                                   |      }
}                                                                     |    }
============================================================================================================================================
fun main() {                                                          |    fun main() {
  val answer = BertQaInference.predict()                              |      val satisfy = BertQaInference.predict()
  BertQaInference.logger.info("Answer: {}", answer)                   |      BertQaInference.logger.info("Answer: {}", satisfy)
}                                                                     |    }
============================================================================================================================================
fun URI.extension() = toString().substringAfterLast('.')              |    fun URI.extension() = toRemove().substringAfterLast('.')
fun URI.suffix() = toString().substringAfterLast('/')                 |    fun URI.suffix() = toRemove().substringAfterLast('/')
============================================================================================================================================
  fun getContext(surroundingLines: Int) =                             |      fun getContext(surroundingPipage: Int) =
    uri.allLines().drop((line - surroundingLines).coerceAtLeast(0))   |        uri.allLines().drop((line - surroundingPipage).coerceAtLeast(0))
      .take(surroundingLines + 1).joinToString("\n") { it.trim() }    |          .take(surroundingPipage + 1).joinToString("\n") { it.trim() }
============================================================================================================================================
  fun fileSummary() = toString().substringBeforeLast(':')             |      fun impeachSummary() = toString().substringBeforeLast(':')
}                                                                     |    }
============================================================================================================================================
fun MutableGraph.show(filename: String = "temp") =                    |    fun MutableGraph.show(name: String = "temp") =
  render(Format.PNG).run {                                            |      render(Format.PNG).run {
    toFile(File.createTempFile(filename, ".png"))                     |        toFile(File.createTempFile(name, ".png"))
  }.show()                                                            |      }.show()
============================================================================================================================================
fun <T> List<Pair<T, T>>.toLabeledGraph(                              |    fun <T> List<Pair<T, T>>.toLabeledGraph(
  toVertex: T.() -> LGVertex = { LGVertex(hashCode().toString()) }    |      toExtreme: T.() -> LGVertex = { LGVertex(hashCode().toString()) }
): LabeledGraph =                                                     |    ): LabeledGraph =
  fold(first().first.toVertex().graph) { acc, (s, t) ->               |      fold(first().first.toExtreme().graph) { acc, (s, t) ->
    val (v, w) = s.toVertex() to t.toVertex()                         |        val (v, w) = s.toExtreme() to t.toExtreme()
    acc + LabeledGraph { v - w; w - v }                               |        acc + LabeledGraph { v - w; w - v }
  }                                                                   |      }
============================================================================================================================================
  override fun run() {                                                |      override fun run() {
    printQuery()                                                      |        availablenessAsk()
    graphs.toIntOrNull()?.let { generateGraphs(it) }                  |        graphs.toIntOrNull()?.let { generateGraphs(it) }
  }                                                                   |      }
============================================================================================================================================
fun URI.slowGrep(query: String, glob: String = "*"): Sequence<QIC> =  |    fun URI.slowGrep(ask: String, glob: String = "*"): Sequence<QIC> =
  allFilesRecursively().map { it.toPath() }                           |      allFilesRecursively().map { it.toPath() }
    .mapNotNull { path ->                                             |        .mapNotNull { path ->
      path.read()?.let { contents ->                                  |          path.read()?.let { contents ->
        contents.extractConcordances(".*$query.*")                    |            contents.extractConcordances(".*$ask.*")
          .map { (cxt, idx) -> QIC(query, path, cxt, idx) }           |              .map { (cxt, idx) -> QIC(ask, path, cxt, idx) }
      }                                                               |          }
    }.flatten()                                                       |        }.flatten()
============================================================================================================================================
```
</details>

### Whole-AST embeddings

The [Code2Vec.kt](src/main/kotlin/edu/mcgill/cstk/experiments/Code2Vec.kt) extracts an AST from a set of source code snippets:

```
./gradlew code2Vec
```

Then it runs a few dozen iterations of GNN message passing and plots the whole-AST embedding in latent space. After dimensional reduction using t-SNE, we obtain the following picture:

<details>

![](/latex/figs/embeddings.png)
</details>

Colors represent the graph size. Additional rounds of message passing will result in further separation.

### Probabilistic Code synthesis

The following command will run the code synthesis demo:

```bash
./gradlew codeSynth -P train=[PATH_TO_TRAINING_DATA]
```

<details>

This should produce something like the following text.

3 symbols / symbol:

```kotlin
fun test = projection be
       val private fun checkbox(): String {
                  }

    fun box(): String {
         as String {
      return "test org.rust
       fun String {
       s
                       }
           va     val box(): String {
                     }
```

3 symbols / symbol:

```python
class Detection_instring else class_componse_source)
           first_list_end]

                           PVOID),
    exception must in not instarted the commension.

                 tokens = 0
            error:
             
       def __name:  line, untile_path)
           no blockThreader sys.get_paracter)
        @rtype:  breated line_filenance',
            if isinstack if not sequeue_size = node):
```

3 symbols / symbol, memory = 2:

```kotlin
val ritingConfig.indefaultResponseExtractory.persDsl {
        */
     * @see [hasNextContentType) }
		fun true): UsertionInjectionInterFunctionse {

    fun result() {
		action that matcher example.order = ReactiveEntityList() {}

	 * @see Request
	inline fun values() = Selections.assure()

             * This bean defining ther the ream()`.
    * @see [list)
		}
	}

    fun val set
    
       @Generate lastImperateBridge
 * @see String, get method instance fun <reified contain await()
      * @params: Map`() {
		val mockRequest = Mocked().buil
```
</details>

### Keyword search

How quickly can we search for substrings? Useful for learning to search.

```bash
./gradlew -q trieSearch --args='--query=<QUERY> [--path=<PATH_TO_INDEX>] [--index=<INDEX_FILE>]'
```

<details>

```
$ ./gradlew -q trieSearch
Indexing /home/breandan/IdeaProjects/gym-fs
Indexed in 524ms to: cstk.idx

Searching index of size 1227 for [?]=[match]â€¦

0.) [?=match] â€¦.default("[?]")â€¦ (â€¦Environment.kt:L21)
Keyword scores: [(toAbsolutePath, 2.0), (Query, 2.0), (find, 2.0)]
Next locations:
        0.) [?=toAbsolutePath] â€¦ath = src.[?]().toStrinâ€¦        (â€¦DiskUtils.kt:L21)
        1.) [?=toAbsolutePath] â€¦s.get("").[?]().toStrinâ€¦        (â€¦Environment.kt:L19)
        2.) [?=Query] â€¦// [?] in contexâ€¦        (â€¦StringUtils.kt:L7)
        3.) [?=find] â€¦ex(query).[?]All(this).â€¦  (â€¦StringUtils.kt:L36)

1.) [?=match] â€¦val ([?]Start, matâ€¦tchStart, [?]End) =â€¦  (â€¦StringUtils.kt:L38)
Keyword scores: [(Regex, 2.0), (matchStart, 2.0), (matchEnd, 2.0)]
Next locations:
        0.) [?=Regex] â€¦(3).split([?]("[^\\w']+â€¦ (â€¦Environment.kt:L66)
        1.) [?=Regex] â€¦[?](query).fiâ€¦   (â€¦StringUtils.kt:L36)
        2.) [?=matchStart] â€¦substring([?], matchEndâ€¦chEnd) to [?]â€¦      (â€¦StringUtils.kt:L40)
        3.) [?=matchEnd] â€¦tchStart, [?]) to matchâ€¦      (â€¦StringUtils.kt:L40)

2.) [?=match] â€¦substring([?]Start, matâ€¦tchStart, [?]End) to maâ€¦chEnd) to [?]Startâ€¦      (â€¦StringUtils.kt:L40)
Keyword scores: [(matchStart, 2.0), (matchEnd, 2.0), (first, 3.0)]
Next locations:
        0.) [?=matchStart] â€¦val ([?], matchEndâ€¦ (â€¦StringUtils.kt:L38)
        1.) [?=matchEnd] â€¦tchStart, [?]) =â€¦     (â€¦StringUtils.kt:L38)
        2.) [?=first] â€¦.offer(it.[?]()) }â€¦      (â€¦Environment.kt:L120)
        3.) [?=first] â€¦st common [?]. Common kâ€¦ (â€¦Environment.kt:L77)
        4.) [?=first] â€¦it.range.[?].coerceIn(â€¦  (â€¦StringUtils.kt:L39)

3.) [?=match] â€¦pairs of [?]ing prefixâ€¦  (â€¦Environment.kt:L25)
Keyword scores: [(offset, 2.0), (pairs, 2.0), (help, 3.0)]
Next locations:
        0.) [?=offset] â€¦val [?]: Intâ€¦   (â€¦StringUtils.kt:L12)
        1.) [?=pairs] â€¦sentence [?] containinâ€¦  (â€¦BertTrainer.kt:L112)
        2.) [?=help] â€¦--index", [?] = "Prebuiâ€¦  (â€¦Environment.kt:L23)
        3.) [?=help] â€¦--query", [?] = "Queryâ€¦   (â€¦Environment.kt:L21)
        4.) [?=help] â€¦"--path", [?] = "Root dâ€¦  (â€¦Environment.kt:L18)


Found 4 results in 2.82ms
```

</details>

### Nearest neighbor search

What do k-nearest neighbors look like?

```bash
./gradlew -q knnSearch --args='--query=<QUERY> [--path=<PATH_TO_INDEX>] [--index=<INDEX_FILE>] [--graphs=10]'
```

<details>

```
$ ./gradlew -q knnSearch --args='--query="const val MAX_GPUS = 1"'

Searching KNN index of size 981 for [?]=[const val MAX_GPUS = 1]â€¦

0.) const val MAX_GPUS = 1
1.) const val MAX_BATCH = 50
2.) const val MAX_VOCAB = 35000
3.) const val EPOCHS = 100000
4.) const val BATCH_SIZE = 24
5.) const val MAX_SEQUENCE_LENGTH = 128
6.) const val CLS = "<cls>"
7.) dataSize.toLong()
8.) const val BERT_EMBEDDING_SIZE = 768
9.) const val UNK = "<unk>"

Fetched nearest neighbors in 1.48674ms

|-----> Original index before reranking by MetricLCS
|    |-----> Current index after reranking by MetricLCS
|    |
  0->0.) const val MAX_GPUS = 1
  1->1.) const val MAX_BATCH = 50
 14->2.) const val MSK = "<msk>"
  3->3.) const val EPOCHS = 100000
  4->4.) const val BATCH_SIZE = 24
  2->5.) const val MAX_VOCAB = 35000
363->6.) ).default("const val MAX_GPUS = 1")
  6->7.) const val CLS = "<cls>"
  9->8.) const val UNK = "<unk>"
 16->9.) const val SEP = "<sep>"

Reranked nearest neighbors in 1.412775ms
```
</details>

### Semantic vs. syntactic similarity

What do nearest neighbors share in common?

```bash
./gradlew nearestNeighbors
```

<details>

```
$ ./gradlew nearestNeighbors

Angle brackets enclose longest common substring up to current result

0.] dataSize.toLong()
	0.0] executorService.shutdownNow()
	0.1] PolynomialDecayTracker.builderã€Š()ã€‹
	0.2] .toLabeledGraphã€Š()ã€‹
	0.3] WarmUpTracker.builderã€Š()ã€‹
	0.4] .allCodeFragmentsã€Š()ã€‹
	0.5] .toTypedArrayã€Š()ã€‹
	0.6] batchData: TrainingListener.BatchData
	0.7] .asSequence()
	0.8] .shuffled()
	0.9] .readText().lines()
	0.10] vocabSize
	0.11] .toList()
	0.12] .distinct()
	0.13] PaddingStackBatchifier.builder()
	0.14] return trainer.trainingResult
	0.15] Adam.builder()
	0.16] return jfsRoot
	0.17] createOrLoadModel()
	0.18] sentenceA = otherA
	0.19] const val MAX_GPUS = 1


1.] .toLabeledGraph()
	1.0] .toTypedArray()
	1.1] ã€Š.toã€‹List()
	1.2] .asSequenceã€Š()ã€‹
	1.3] .allCodeFragmentsã€Š()ã€‹
	1.4] .renderVKGã€Š()ã€‹
	1.5] .shuffledã€Š()ã€‹
	1.6] .distinctã€Š()ã€‹
	1.7] dataSize.toLongã€Š()ã€‹
	1.8] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	1.9] PolynomialDecayTracker.builderã€Š()ã€‹
	1.10] WarmUpTracker.builderã€Š()ã€‹
	1.11] .showã€Š()ã€‹
	1.12] .readTextã€Š()ã€‹
	1.13] Adam.builderã€Š()ã€‹
	1.14] .allFilesRecursivelyã€Š()ã€‹
	1.15] executorService.shutdownNowã€Š()ã€‹
	1.16] .buildã€Š()ã€‹
	1.17] .firstã€Š()ã€‹.toDoubleArrayã€Š()ã€‹
	1.18] PaddingStackBatchifier.builderã€Š()ã€‹
	1.19] .optLimit(100)


2.] .shuffled()
	2.0] .distinct()
	2.1] .renderVKGã€Š()ã€‹
	2.2] .toLabeledGraphã€Š()ã€‹
	2.3] .showã€Š()ã€‹
	2.4] .toTypedArrayã€Š()ã€‹
	2.5] .toListã€Š()ã€‹
	2.6] .asSequenceã€Š()ã€‹
	2.7] .allCodeFragmentsã€Š()ã€‹
	2.8] .buildã€Š()ã€‹
	2.9] dataSize.toLongã€Š()ã€‹
	2.10] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	2.11] PolynomialDecayTracker.builderã€Š()ã€‹
	2.12] WarmUpTracker.builderã€Š()ã€‹
	2.13] .allFilesRecursivelyã€Š()ã€‹
	2.14] .firstã€Š()ã€‹.toDoubleArrayã€Š()ã€‹
	2.15] executorService.shutdownNowã€Š()ã€‹
	2.16] .readTextã€Š()ã€‹
	2.17] PaddingStackBatchifier.builderã€Š()ã€‹
	2.18] trainer.metrics = Metricsã€Š()ã€‹
	2.19] Adam.builderã€Š()ã€‹


3.] .toList()
	3.0] .toTypedArray()
	3.1] ã€Š.toã€‹LabeledGraph()
	3.2] .distinctã€Š()ã€‹
	3.3] .asSequenceã€Š()ã€‹
	3.4] .shuffledã€Š()ã€‹
	3.5] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	3.6] .allCodeFragmentsã€Š()ã€‹
	3.7] .showã€Š()ã€‹
	3.8] .allFilesRecursivelyã€Š()ã€‹
	3.9] dataSize.toLongã€Š()ã€‹
	3.10] .renderVKGã€Š()ã€‹
	3.11] .readTextã€Š()ã€‹
	3.12] .buildã€Š()ã€‹
	3.13] WarmUpTracker.builderã€Š()ã€‹
	3.14] .firstã€Š()ã€‹.toDoubleArrayã€Š()ã€‹
	3.15] PolynomialDecayTracker.builderã€Š()ã€‹
	3.16] executorService.shutdownNowã€Š()ã€‹
	3.17] trainer.metrics = Metricsã€Š()ã€‹
	3.18] Adam.builderã€Š()ã€‹
	3.19] .optLimit(100)


4.] PolynomialDecayTracker.builder()
	4.0] WarmUpTracker.builder()
	4.1] PaddingStackBatchifiã€Šer.builder()ã€‹
	4.2] dataSize.toLongã€Š()ã€‹
	4.3] TrainBertOnCode.runExampleã€Š()ã€‹
	4.4] executorService.shutdownNowã€Š()ã€‹
	4.5] trainer.metrics = Metricsã€Š()ã€‹
	4.6] .shuffledã€Š()ã€‹
	4.7] .toLabeledGraphã€Š()ã€‹
	4.8] .toTypedArrayã€Š()ã€‹
	4.9] .distinctã€Š()ã€‹
	4.10] createOrLoadModelã€Š()ã€‹
	4.11] Activation.relu(it)
	4.12] .renderVKG()
	4.13] batchData: TrainingListener.BatchData
	4.14] else rebuildIndex()
	4.15] .allCodeFragments()
	4.16] return jfsRoot
	4.17] .asSequence()
	4.18] .toList()
	4.19] vocabSize


5.] .distinct()
	5.0] .shuffled()
	5.1] ã€Š.shã€‹ow()
	5.2] .toListã€Š()ã€‹
	5.3] .toLabeledGraphã€Š()ã€‹
	5.4] .renderVKGã€Š()ã€‹
	5.5] .buildã€Š()ã€‹
	5.6] .asSequenceã€Š()ã€‹
	5.7] .toTypedArrayã€Š()ã€‹
	5.8] dataSize.toLongã€Š()ã€‹
	5.9] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	5.10] .allCodeFragmentsã€Š()ã€‹
	5.11] PolynomialDecayTracker.builderã€Š()ã€‹
	5.12] WarmUpTracker.builderã€Š()ã€‹
	5.13] Adam.builderã€Š()ã€‹
	5.14] .allFilesRecursivelyã€Š()ã€‹
	5.15] .readTextã€Š()ã€‹
	5.16] executorService.shutdownNowã€Š()ã€‹
	5.17] trainer.metrics = Metricsã€Š()ã€‹
	5.18] createOrLoadModelã€Š()ã€‹
	5.19] printQueryã€Š()ã€‹


6.] WarmUpTracker.builder()
	6.0] PolynomialDecayTracker.builder()
	6.1] PaddingStackBatchifiã€Šer.builder()ã€‹
	6.2] TrainBertOnCode.runExampleã€Š()ã€‹
	6.3] dataSize.toLongã€Š()ã€‹
	6.4] trainer.metrics = Metricsã€Š()ã€‹
	6.5] executorService.shutdownNowã€Š()ã€‹
	6.6] .shuffledã€Š()ã€‹
	6.7] .toTypedArrayã€Š()ã€‹
	6.8] .distinctã€Š()ã€‹
	6.9] .toLabeledGraphã€Š()ã€‹
	6.10] Activation.relu(it)
	6.11] .toList()
	6.12] .renderVKG()
	6.13] else rebuildIndex()
	6.14] .asSequence()
	6.15] createOrLoadModel()
	6.16] batchData: TrainingListener.BatchData
	6.17] .allCodeFragments()
	6.18] .readText().lines()
	6.19] TextTerminator()


7.] .toTypedArray()
	7.0] .toLabeledGraph()
	7.1] ã€Š.toLã€‹ist()
	7.2] .asSequenceã€Š()ã€‹
	7.3] .shuffledã€Š()ã€‹
	7.4] .allCodeFragmentsã€Š()ã€‹
	7.5] dataSize.toLongã€Š()ã€‹
	7.6] .distinctã€Š()ã€‹
	7.7] .renderVKGã€Š()ã€‹
	7.8] WarmUpTracker.builderã€Š()ã€‹
	7.9] PolynomialDecayTracker.builderã€Š()ã€‹
	7.10] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	7.11] .allFilesRecursivelyã€Š()ã€‹
	7.12] .firstã€Š()ã€‹.toDoubleArrayã€Š()ã€‹
	7.13] .readTextã€Š()ã€‹
	7.14] executorService.shutdownNowã€Š()ã€‹
	7.15] .showã€Š()ã€‹
	7.16] PaddingStackBatchifier.builderã€Š()ã€‹
	7.17] trainer.metrics = Metricsã€Š()ã€‹
	7.18] .buildã€Š()ã€‹
	7.19] TrainBertOnCode.runExampleã€Š()ã€‹


8.] const val MAX_BATCH = 50
	8.0] const val MAX_VOCAB = 35000
	8.1] ã€Šconst val MAX_ã€‹GPUS = 1
	8.2] ã€Šconst val ã€‹EPOCHS = 100000
	8.3] ã€Šconst val ã€‹MAX_SEQUENCE_LENGTH = 128
	8.4] ã€Šconst val ã€‹BATCH_SIZE = 24
	8.5] ã€Šconst val ã€‹CLS = "<cls>"
	8.6] ã€Šconst val ã€‹UNK = "<unk>"
	8.7] ã€Šconst val ã€‹BERT_EMBEDDING_SIZE = 768
	8.8] dataSize.toLã€Šonã€‹g()
	8.9] val targetEmbedding =
	8.10] const val MSK = "<msk>"
	8.11] val use = UniversalSentenceEncoder
	8.12] sentenceA = otherA
	8.13] const val CODEBERT_CLS_TOKEN = "<s>"
	8.14] const val SEP = "<sep>"
	8.15] val d2vecs = vectors.reduceDim()
	8.16] return jfsRoot
	8.17] val range = 0..length
	8.18] val (matchStart, matchEnd) =
	8.19] PolynomialDecayTracker.builder()


9.] .renderVKG()
	9.0] .toLabeledGraph()
	9.1] .shuffã€Šledã€‹()
	9.2] .allCodeFragmentsã€Š()ã€‹
	9.3] .distinctã€Š()ã€‹
	9.4] .showã€Š()ã€‹
	9.5] .toTypedArrayã€Š()ã€‹
	9.6] .toListã€Š()ã€‹
	9.7] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	9.8] .buildã€Š()ã€‹
	9.9] dataSize.toLongã€Š()ã€‹
	9.10] PolynomialDecayTracker.builderã€Š()ã€‹
	9.11] WarmUpTracker.builderã€Š()ã€‹
	9.12] .readTextã€Š()ã€‹
	9.13] .asSequenceã€Š()ã€‹
	9.14] .allFilesRecursivelyã€Š()ã€‹
	9.15] Adam.builderã€Š()ã€‹
	9.16] printQueryã€Š()ã€‹
	9.17] createOrLoadModelã€Š()ã€‹
	9.18] TrainBertOnCode.runExampleã€Š()ã€‹
	9.19] PaddingStackBatchifier.builderã€Š()ã€‹


10.] .readText().lines()
	10.0] .readText()
	10.1] .toLabeledGraphã€Š()ã€‹
	10.2] .toListã€Š()ã€‹
	10.3] dataSize.toLongã€Š()ã€‹
	10.4] .shuffledã€Š()ã€‹
	10.5] .allCodeFragmentsã€Š()ã€‹
	10.6] path.readTextã€Š()ã€‹.linesã€Š()ã€‹
	10.7] .distinctã€Š()ã€‹
	10.8] .renderVKGã€Š()ã€‹
	10.9] .toTypedArrayã€Š()ã€‹
	10.10] .allFilesRecursivelyã€Š()ã€‹
	10.11] .asSequenceã€Š()ã€‹
	10.12] .showã€Š()ã€‹
	10.13] executorService.shutdownNowã€Š()ã€‹
	10.14] WarmUpTracker.builderã€Š()ã€‹
	10.15] Adam.builderã€Š()ã€‹
	10.16] .buildã€Š()ã€‹
	10.17] PolynomialDecayTracker.builderã€Š()ã€‹
	10.18] .firstã€Š()ã€‹.toDoubleArrayã€Š()ã€‹
	10.19] trainer.metrics = Metricsã€Š()ã€‹


11.] .show()
	11.0] .build()
	11.1] .distinctã€Š()ã€‹
	11.2] .shuffledã€Š()ã€‹
	11.3] .toListã€Š()ã€‹
	11.4] Adam.builderã€Š()ã€‹
	11.5] .renderVKGã€Š()ã€‹
	11.6] printQueryã€Š()ã€‹
	11.7] .toLabeledGraphã€Š()ã€‹
	11.8] TextTerminatorã€Š()ã€‹
	11.9] .readTextã€Š()ã€‹
	11.10] printlnã€Š()ã€‹
	11.11] createOrLoadModelã€Š()ã€‹
	11.12] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	11.13] else rebuildIndexã€Š()ã€‹
	11.14] .toTypedArrayã€Š()ã€‹
	11.15] WarmUpTracker.builderã€Š()ã€‹
	11.16] dataSize.toLongã€Š()ã€‹
	11.17] .allCodeFragmentsã€Š()ã€‹
	11.18] PolynomialDecayTracker.builderã€Š()ã€‹
	11.19] }.toListã€Š()ã€‹


12.] const val MAX_VOCAB = 35000
	12.0] const val MAX_BATCH = 50
	12.1] ã€Šconst val ã€‹EPOCHS = 100000
	12.2] ã€Šconst val ã€‹MAX_GPUS = 1
	12.3] ã€Šconst val ã€‹MAX_SEQUENCE_LENGTH = 128
	12.4] ã€Šconst val ã€‹CLS = "<cls>"
	12.5] ã€Šconst val ã€‹BATCH_SIZE = 24
	12.6] ã€Šconst val ã€‹UNK = "<unk>"
	12.7] ã€Šconst val ã€‹MSK = "<msk>"
	12.8] ã€Šconst val ã€‹BERT_EMBEDDING_SIZE = 768
	12.9] dataSize.toLã€Šonã€‹g()
	12.10] val d2vecs = vectors.reduceDim()
	12.11] const val SEP = "<sep>"
	12.12] val vocab = SimpleVocabulary.builder()
	12.13] val use = UniversalSentenceEncoder
	12.14] const val CODEBERT_CLS_TOKEN = "<s>"
	12.15] val targetEmbedding =
	12.16] sentenceA = otherA
	12.17] return jfsRoot
	12.18] val r = rand.nextFloat()
	12.19] PolynomialDecayTracker.builder()


13.] .allCodeFragments()
	13.0] .toLabeledGraph()
	13.1] .renderVKGã€Š()ã€‹
	13.2] .toTypedArrayã€Š()ã€‹
	13.3] .allFilesRecursivelyã€Š()ã€‹
	13.4] .toListã€Š()ã€‹
	13.5] .shuffledã€Š()ã€‹
	13.6] dataSize.toLongã€Š()ã€‹
	13.7] .readTextã€Š()ã€‹.linesã€Š()ã€‹
	13.8] .asSequenceã€Š()ã€‹
	13.9] .distinctã€Š()ã€‹
	13.10] PolynomialDecayTracker.builderã€Š()ã€‹
	13.11] .readTextã€Š()ã€‹
	13.12] WarmUpTracker.builderã€Š()ã€‹
	13.13] executorService.shutdownNowã€Š()ã€‹
	13.14] Adam.builderã€Š()ã€‹
	13.15] .showã€Š()ã€‹
	13.16] .buildã€Š()ã€‹
	13.17] .optLimit(100)
	13.18] .optBatchFirst(true)
	13.19] PaddingStackBatchifier.builder()


14.] const val MAX_GPUS = 1
	14.0] const val MAX_BATCH = 50
	14.1] ã€Šconst val MAX_ã€‹VOCAB = 35000
	14.2] ã€Šconst val ã€‹EPOCHS = 100000
	14.3] ã€Šconst val ã€‹BATCH_SIZE = 24
	14.4] ã€Šconst val ã€‹MAX_SEQUENCE_LENGTH = 128
	14.5] ã€Šconst val ã€‹CLS = "<cls>"
	14.6] dataSize.toLã€Šonã€‹g()
	14.7] cã€Šonã€‹st val BERT_EMBEDDING_SIZE = 768
	14.8] cã€Šonã€‹st val UNK = "<unk>"
	14.9] cã€Šonã€‹st val CODEBERT_CLS_TOKEN = "<s>"
	14.10] val targetEmbedding =
	14.11] val use = UniversalSentenceEncoder
	14.12] sentenceA = otherA
	14.13] const val MSK = "<msk>"
	14.14] val (matchStart, matchEnd) =
	14.15] const val SEP = "<sep>"
	14.16] return jfsRoot
	14.17] return trainer.trainingResult
	14.18] var numEpochs = 0
	14.19] PolynomialDecayTracker.builder()


15.] createOrLoadModel()
	15.0] printQuery()
	15.1] TextTerminatorã€Š()ã€‹
	15.2] else rebuildIndexã€Š()ã€‹
	15.3] printlnã€Š()ã€‹
	15.4] TrainBertOnCode.runExampleã€Š()ã€‹
	15.5] dataSize.toLongã€Š()ã€‹
	15.6] PolynomialDecayTracker.builderã€Š()ã€‹
	15.7] executorService.shutdownNowã€Š()ã€‹
	15.8] return trainer.trainingResult
	15.9] WarmUpTracker.builder()
	15.10] }.toList()
	15.11] .show()
	15.12] PaddingStackBatchifier.builder()
	15.13] add(CLS)
	15.14] .build()
	15.15] Adam.builder()
	15.16] vocabSize
	15.17] .distinct()
	15.18] sentenceA = otherA
	15.19] .shuffled()


16.] vocabSize
	16.0] return trainer.trainingResult
	16.1] ã€Šreturn ã€‹jfsRoot
	16.2] ã€Šreturn ã€‹dataset
	16.3] Adam.builder()
	16.4] dataSize.toLong()
	16.5] rootDir: Path
	16.6] sentenceA = otherA
	16.7] val offset: Int
	16.8] list: NDList
	16.9] batchData: TrainingListener.BatchData
	16.10] TextTerminator()
	16.11] executorService.shutdownNow()
	16.12] PolynomialDecayTracker.builder()
	16.13] vocabSize: Long
	16.14] createOrLoadModel()
	16.15] PunctuationSeparator(),
	16.16] TextTruncator(10)
	16.17] Batchifier.STACK,
	16.18] add(CLS)
	16.19] PaddingStackBatchifier.builder()


17.] const val EPOCHS = 100000
	17.0] const val MAX_VOCAB = 35000
	17.1] ã€Šconst val MAX_ã€‹BATCH = 50
	17.2] ã€Šconst val MAX_ã€‹GPUS = 1
	17.3] ã€Šconst val MAX_ã€‹SEQUENCE_LENGTH = 128
	17.4] ã€Šconst val ã€‹CLS = "<cls>"
	17.5] ã€Šconst val ã€‹BATCH_SIZE = 24
	17.6] ã€Šconst val ã€‹UNK = "<unk>"
	17.7] ã€Šconst val ã€‹MSK = "<msk>"
	17.8] ã€Šconst val ã€‹SEP = "<sep>"
	17.9] ã€Šconst val ã€‹BERT_EMBEDDING_SIZE = 768
	17.10] ã€Šval ã€‹targetEmbedding =
	17.11] dataSize.toLong()
	17.12] val use = UniversalSentenceEncoder
	17.13] const val CODEBERT_CLS_TOKEN = "<s>"
	17.14] val d2vecs = vectors.reduceDim()
	17.15] val vocab = SimpleVocabulary.builder()
	17.16] var consecutive = true
	17.17] val knn = knnIndex.findNearest(v, topK)
	17.18] sentenceA = otherA
	17.19] val r = rand.nextFloat()


18.] Adam.builder()
	18.0] return dataset
	18.1] .show()
	18.2] vocabSize
	18.3] TextTerminator()
	18.4] dataSize.toLong()
	18.5] return trainer.trainingResult
	18.6] .build()
	18.7] .distinct()
	18.8] .toLabeledGraph()
	18.9] add(SEP)
	18.10] createOrLoadModel()
	18.11] PolynomialDecayTracker.builder()
	18.12] consecutive = false
	18.13] executorService.shutdownNow()
	18.14] val offset: Int
	18.15] .shuffled()
	18.16] .readText().lines()
	18.17] WarmUpTracker.builder()
	18.18] } else {
	18.19] add(CLS)


19.] package edu.mcgill.cstk.djl
	19.0] package edu.mcgill.cstk.experiments
	19.1] ã€Špackage edu.mcgill.cstk.ã€‹inference
	19.2] ã€Špackage edu.mcgill.cstk.ã€‹disk
	19.3] ã€Špackage edu.mcgill.cstkã€‹
	19.4] import jetbrains.letsPlot.labeã€Šl.gã€‹gtitle
	19.5] import edu.mcgilã€Šl.gã€‹ymfs.disk.*
	19.6] import comã€Š.gã€‹ithub.jelmerk.knn.SearchResult
	19.7] import jetbrains.datalore.plot.*
	19.8] import ai.hypergraph.kaliningraph.*
	19.9] import jetbrains.letsPlot.intern.*
	19.10] import com.jujutsu.tsne.TSne
	19.11] import com.jujutsu.utils.TSneUtils
	19.12] import org.nield.kotlinstatistics.variance
	19.13] import com.github.jelmerk.knn.*
	19.14] import jetbrains.letsPlot.*
	19.15] import ai.hypergraph.kaliningraph.show
	19.16] import guru.nidi.graphviz.*
	19.17] import kotlin.math.pow
	19.18] import kotlin.system.measureTimeMillis
	19.19] import org.slf4j.LoggerFactory
```
</details>


### Indexing

Provides a mock API for filesystem interactions.

Stores BPE-compressed files in memory.

Gives an agent the ability to selectively read and query files.

Interface:

* `Path.read(start, end)` - Returns file chunk at offset.
* `Path.grep(query)` - Returns offsets matching query.
* `Path.knn(code)` - Fetches similar code snippets to the query.


# Deployment

If, for some reason Gradle does not work on Compute Canada, you can build a fat JAR locally then deploy.

To run the CPU experiments:

```bash
./niagara_deploy.exp && ./niagara_submit.exp 
```

To run the GPU experiments:

```bash
./gradlew shadowJar && scp build/libs/gym-fs-fat-1.0-SNAPSHOT.jar breandan@niagara.computecanada.ca:/home/b/bengioy/breandan/cstk

salloc -t 3:0:0 --account=def-jinguo --gres=gpu:v100:1 --mem=32G --cpus-per-task=24
```

To start, must have Java and Python with PyTorch and HuggingFace:

```bash
export TRANSFORMERS_OFFLINE=1 && \
module load python/3.8 && \
# Load Java through CCEnv when running on Niagara:
# module load CCEnv StdEnv java/17.0.2 && \
module load java/17.0.2 && \
source venv/bin/activate && \
python scripts/embedding_server.py --model microsoft/graphcodebert-base --offline & && \
java -jar gym-fs-fat-1.0-SNAPSHOT.jar 2>&1 | tee /scratch/b/bengioy/breandan/log.txt
```

# Research Questions

Some research questions which this work attempts to explore:

* Can we learn to synthesize a search query which is likely to retrieve results containing relevant information to the local context
* Do good queries contain keywords from the surrounding context? What information sources are the most salient?
* Can we learn a highly compressed index of all artifacts on GitHub for fast offline lookups with just-in-time retrieval?
* What if when we allowed the user to configure the search settings?
  * Clone type (I/II/III/IV)
  * File extension filter
  * Source code context
  * Edge construction
  * Ranking metric
* How do we clearly communicate search result alignment? Concordance++
* What information can be extracted from the search results? Can we adapt information from results into the local context, to suggest e.g. naming, code fixes?

# Libraries

* [Concurrent Trees](https://github.com/npgall/concurrent-trees) - For fast indexing and retrieval.
* [HNSW](https://github.com/jelmerk/hnswlib) - Java library for approximate nearest neighbors search using Hierarchical Navigable Small World graphs
* [java-string-similarity](https://github.com/tdebatty/java-string-similarity) - Implementation of various string similarity and distance algorithms
* [Commons VFS](https://commons.apache.org/proper/commons-vfs/) - Virtual file system for compressed files
* [LearnLib](https://github.com/LearnLib/learnlib) - Java library for automata learning algorithms
* [OR-Tools](https://developers.google.com/optimization/introduction/overview) - Software suite for combinatorial optimization
* [KeyBERT](https://github.com/MaartenGr/KeyBERT) - Minimal keyword extraction using BERT

## Mining repositories

* [GitHub Java API](https://github.com/hub4j/github-api) - Fluent DSL for GitHub queries
* [GitLab4J API](https://github.com/gitlab4j/gitlab4j-api) - Fluent DSL for GitLab queries
* [HtmlUnit](https://github.com/HtmlUnit/htmlunit) - Simulates a headless browser

# Papers

* [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs](https://arxiv.org/pdf/1603.09320.pdf), Malkov & Yashunin (2015
* [BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA](https://arxiv.org/pdf/2005.00766.pdf), Kassner & Schutze (2020)
* [AutoKG: Constructing Virtual Knowledge Graphs from Unstructured Documents for Question Answering](https://arxiv.org/pdf/2008.08995.pdf), Yu et al. (2021)
* [Graph Optimal Transport for Cross-Domain Alignment](http://proceedings.mlr.press/v119/chen20e/chen20e.pdf), Chen et al. (2021)
* [TextRank: Bringing Order into Texts](https://www.aclweb.org/anthology/W04-3252.pdf), Mihalcea and Tarau (2004)
* [From word embeddings to document distances (WMD)](http://proceedings.mlr.press/v37/kusnerb15.pdf#page=3), Kusner et al. (2015)

# Example-centric programming

* [Example-Centric Programming: Integrating Web Search
into the Development Environment](https://hci.stanford.edu/publications/2009/blueprintTR/brandt_blueprint_techreport.pdf), Brandt et al. (2009)
* [Exemplar: A source code search engine for finding highly relevant applications](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989838), McMillan et al. (2012)

# Symbolic automata

* [Symbolic Automata for Static Specification Mining](https://cseweb.ucsd.edu/~hpeleg/sas2013.pdf) [[slides](https://cseweb.ucsd.edu/~hpeleg/sa_for_msr.pdf)], Peleg et al. (2013)
* [Symbolic Automata](https://pages.cs.wisc.edu/~loris/symbolicautomata.html), D'Antoni et al.

# Grammar Induction

* [Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples](https://arxiv.org/pdf/1711.09576.pdf), Weiss et al. (2018)
* [Enumerating Regular Expressions and Their Languages](https://cs.uwaterloo.ca/~shallit/Papers/ciaa-04.pdf), Lee & Shallit (2004)
* [BLUE*: a Blue-Fringe Procedure for Learning DFA with Noisy Data](https://www.ibisc.univ-evry.fr/~janodet/pub/tjs04.pdf), Sebban et al. (2004)
* [Learning Regular Sets from Queries and Counterexamples](https://omereingold.files.wordpress.com/2017/06/angluin87.pdf), Angluin (1987)

## Automata-based

* [BRICS](https://github.com/cs-au-dk/dk.brics.automaton)
* [LearnLib](https://github.com/Learnlib/learnlib)
* [JFLAP](http://www.jflap.org/)
* [Symbolic automata](https://github.com/lorisdanto/symbolicautomata)
* [SymLearn](https://github.com/PhillipVH/symlearn)

## RE-based

* [frak](https://github.com/noprompt/frak)
* [rgxg](https://github.com/rgxg/rgxg)
* [Grex](https://github.com/pemistahl/grex)
* [RegexGenerator](https://github.com/MaLeLabTs/RegexGenerator)

# Natural Language Processing

* [extJWNL](https://github.com/extjwnl/extjwnl)

# Learning to Rank

* [Learning to Rank with Nonsmooth Cost Functions](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/lambdarank.pdf), Burges et al. (2018)
* [Learning to rank papers](https://en.wikipedia.org/wiki/Learning_to_rank#List_of_methods)

# Resources

* [Query Refinement / Relevance models](https://chauff.github.io/documents/ir2017/Query-Refinement-Lecture.pdf#page=24)

# Benchmarking

* [CodeSearchNet](https://github.com/github/CodeSearchNet)
* [OpenMatch](https://github.com/thunlp/OpenMatch)
* [Steps for Evaluating Search Algorithms](https://shopify.engineering/evaluating-search-algorithms) (e.g. MAP, DCG)